<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Convex Optimized feed.</title>
        <link>http://zinkov.com</link>
        <description><![CDATA[Various ruminations on machine learning and mathematics]]></description>
        <atom:link href="http://zinkov.com/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Sun, 17 Feb 2019 00:00:00 UT</lastBuildDate>
        <item>
    <title>Why care about Program Synthesis</title>
    <link>http://zinkov.com/posts/2019-02-17-why-program-synthesis/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Why care about Program Synthesis</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Why care about Program Synthesis</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2019-02-17</h5>
</div>

<div class="text-left">

<p>Program synthesis is now emerging as an exciting new area of research not just in the programming languages community, but also the machine learning community. In this post, I’d like to convince you why this area of study has the potential to solve precisely the kinds of problems existing approaches built around differential programming struggle with.</p>
<h2 id="basics-of-program-synthesis">Basics of Program Synthesis</h2>
<p>To start let’s informally and somewhat formally define what makes something a program synthesis problem. Informally, program synthesis is where given a some language <span class="math inline">\(\mathcal{L}\)</span> and specification <span class="math inline">\(\mathcal{S}\)</span> we return a program <span class="math inline">\(\mathcal{P} \in \mathcal{L}\)</span> which meets that specification.</p>
<p>So what languages (<span class="math inline">\(\mathcal{L}\)</span>) will we use? In principle, any language can be used. So we can synthesize Python code. In practice, because it is difficult these days to create programs much longer than 20-30 lines of code, we concentrate on domain-specific languages (DSLs). DSLs are languages like SQL, Regexes, or OpenGL shaders. If we are willing to be a bit loose about what defines a language, this can include synthesizing a set of library calls like <a href="https://autopandas.io/">Autopandas</a>. All the matters is we can define a grammar that covers the space of programs we wish to consider.</p>
<pre>
   &lt;regex&gt; ::= &lt;term&gt; '|' &lt;regex&gt;
            |  &lt;term&gt;

   &lt;term&gt; ::= { &lt;factor&gt; }

   &lt;factor&gt; ::= &lt;base&gt; { '*' }
             
   &lt;base&gt; ::= &lt;char&gt;
           |  '\' &lt;char&gt;
           |  '(' &lt;regex&gt; ')'  
</pre>
<p><strong>Regex grammar</strong></p>
<p><img src="../../images/grammar_graphics.png" /></p>
<p>What do we mean by a specification (<span class="math inline">\(\mathcal{S}\)</span>)?</p>
<p>This can actually be a wide variety of things. <span class="math inline">\(\mathcal{S}\)</span> can be in particular order one or more of the following:</p>
<ul>
<li>A formal specification of the problem including things like theorems that must be proved along with other formal verification steps.</li>
<li>A set of input/output examples</li>
<li>A set of unit tests and <a href="https://hypothesis.works/articles/what-is-property-based-testing/">property-based</a> tests</li>
<li>A natural language description of the problem</li>
<li>A set of execution traces of the desired program</li>
<li>A sketch of a program where we have a partial program and some blanks we would like to fill in</li>
<li>A correct but inefficient implementation of the desire program</li>
</ul>
<p>While not strictly necessary, we may also have some side information like:</p>
<ul>
<li>Similar but incorrect programs</li>
<li>A set of other programs in <span class="math inline">\(\mathcal{L}\)</span></li>
</ul>
<p>If we restrict ourselves to a specification that consists of input/output examples and a language of pure functions we get something pretty similar to supervised machine learning. But because the specification can be much richer we actually tackle problems that are hard to pose in a way amendable to traditional machine learning algorithms.</p>
<h2 id="program-synthesis-is-good-for">Program synthesis is good for</h2>
<h3 id="introduction">Introduction</h3>
<p>Now while it is a nice generic formalism that isn’t very compelling if there aren’t problems that benefit from being posed that way. Deep Learning and other optimization methods can now be used to solve a diverse set of problems. What problems tend to easier to solve with program syntheis? As things stand today that main advantages of specifically wanting to generate a program have to do with <em>interpretability</em>, <em>generalisability</em>, <em>verification</em>, <em>combinatorial problems</em>, and <em>output needs to be a program</em>.</p>
<h3 id="interpretability">Interpretability</h3>
<p>Consider the task of automatically grading assignments. How would you go about doing this? You might treat this as a classification task where you find the errors. The challenge with this problem is there can be multiple valid solutions, and the fix for the assignment will depend on which solution you think the student was attempting.</p>
<p>Instead, we can synthesize the correct program but exploring the space of small edits that get us from the incorrect program to a correct program that satisfies an already written specification. These edits can then be presented to the student. This is precisely what the paper <a href="https://arxiv.org/abs/1204.1751">Automated Feedback Generation for Introductory Programming Assignments</a> does on a subset of the Python language, and the paper <a href="https://openreview.net/pdf?id=B1iZRFkwz">Towards Specification-Directed Program Repair</a> which does it for the robot manipulation DSL Karel.</p>
<p>If we didn’t treat this as a program we would have likely ended up with some character edits which as much less interpretable.</p>
<p>This can be seen more strikingly in <a href="https://arxiv.org/abs/1707.09627">Learning to Infer Graphics Programs from Hand-Drawn Images</a> where the program we learn in being a program better communicates the structure in the image.</p>
<p><img src="../../images/infer_graphics.png" /></p>
<h3 id="generalisability">Generalisability</h3>
<p>Many deep learning models struggle with generalisibility. They tend not to be very robust to small distribution differences between the training and the testing set as well as being prone to adversarial examples where small imperceptible changes to the input radically change the prediction.</p>
<p>But for many domains if we represent our function as a program it can be made more robust to perturbations of the input like that as can be seen in <a href="https://arxiv.org/abs/1707.09627">Learning to Infer Graphics Programs from Hand-Drawn Images</a></p>
<p>There are actually particular challenges that face the most popular machine learning models which give program synthesis approaches no problems. We know LSTM have trouble with copy and reverse functions as seen in the <a href="https://deepmind.com/blog/article/differentiable-neural-computers">Differentiable Neural computers</a> paper.</p>
<p>LSTM models have trouble generalising to test data longer than training data as can be seen in <a href="https://arxiv.org/abs/1904.11694">Neural Logic Machines</a></p>
<p>In contrast the papers <a href="https://arxiv.org/abs/1704.06611">Making Neural Programming Architectures Generalize via Recursion</a> and <a href="https://arxiv.org/abs/1706.01284">Towards Synthesizing Complex Programs from Input-Output Examples</a> show no issues with either of those tasks.</p>
<p><img src="../../images/genres1.png" /></p>
<h3 id="verification">Verification</h3>
<p>Another advantage comes from our output artifact from a program. Neural networks are difficult to formally verify and at present often require major restrictions be placed on the models. In contrast, with programs we can reuse existing infrastructure for verifying deterministic programs. We can thus verify these programs terminate or obey a formal spec. In some domains like robotics we can check if the program has controlability.</p>
<h3 id="problems-with-combinatorial-shape">Problems with combinatorial shape</h3>
<p>Problems that require dealing with graphs, trees, and permutations still remain fairly challenging for existing machine learning algorithms. Programs are a natural representation for manipulating combinatorial structures. <a href="https://arxiv.org/abs/1506.03134">Pointer networks</a>, <a href="https://arxiv.org/abs/1802.08665">Sinkhorn networks</a> along with work with Memory networks and Neural Turing Machines shows that at the moment it is difficult to learn a function that can handle anything beyond toy problems which themselves have trouble generalizing to larger domains.</p>
<h3 id="required-to-use-some-api-output-must-be-program">Required to use some api / output must be program</h3>
<p>And finally, sometimes for one reason or another you need an output that must satisfy some grammar. This might be learning to generate a phone number or a URL. We might have some API we need to conform like if we are trying to generate mobile software that needs to call out to Android or IOS primitives.</p>
<p>We could be using program synthesis for compiler optimization so we must generate a valid program as output. We could be learning to <a href="https://www.sri.inf.ethz.ch/publications/raychev2015predicting">deobfuscate code</a>. Or learning to generate code that would automatically <a href="https://security.ece.cmu.edu/aeg/aeg-current.pdf">hack a system</a>.</p>
<p>Any other approach will need to model the grammar to make output that is acceptable and at that point could also be argued is performing program synthesis.</p>
<h2 id="conclusions">Conclusions</h2>
<p>None of this is meant to say that these problems couldn’t be solved with other methods, but program synthesis has distinct advantages that enables them to solve them particularly well.</p>

</div>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Sun, 17 Feb 2019 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2019-02-17-why-program-synthesis/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>NeurIPS 2018: Papers to check out</title>
    <link>http://zinkov.com/posts/2018-12-21-nips-reviews/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>NeurIPS 2018: Papers to check out</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>NeurIPS 2018: Papers to check out</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2018-12-21</h5>
</div>

<div class="text-left">

<p>It’s been a long time since I’ve done one of these, so below are some of the papers I found exciting at this past NeurIPS. One notable thing is as the conference has gotten larger is there are simply more papers being presented. Some people worry that in becoming a gigantic conference the quality is declining but thanks to the great efforts of community, more people has meant more interesting ideas and more great papers to read. I typically highlight 5 papers</p>
<h4 id="approximate-inference"><a href="#approx">Approximate Inference</a></h4>
<h4 id="program-synthesis"><a href="#progsynth">Program Synthesis</a></h4>
<h4 id="applications"><a href="#app">Applications</a></h4>
<h4 id="misc"><a href="#misc">Misc</a></h4>
<hr />
<h2 id="conference-papers">Conference papers:</h2>
<a name="approx">
<h3>
Approximate Inference
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/7326-implicit-reparameterization-gradients"><strong>Implicit Reparameterization Gradients</strong></a></p>
<p><em>Mikhail Figurnov, Shakir Mohamed, Andriy Mnih</em></p>
<p><strong>Abstract:</strong> By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.</p>
<p><a href="http://papers.nips.cc/paper/7460-random-feature-stein-discrepancies"><strong>Random Feature Stein Discrepancies</strong></a></p>
<p><em>Jonathan Huggins, Lester Mackey</em></p>
<p><strong>Abstract:</strong> Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power—even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies (ΦSDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct ΦSDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations—random ΦSDs (RΦSDs)—which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, RΦSDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.</p>
<p><a href="http://papers.nips.cc/paper/7514-wasserstein-variational-inference"><strong>Wasserstein Variational Inference</strong></a></p>
<p><em>Luca Ambrogioni, Umut Güçlü, Yağmur Güçlütürk, Max Hinne, Marcel A. J. van Gerven, Eric Maris</em></p>
<p><strong>Abstract:</strong> This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.</p>
<p><a href="http://papers.nips.cc/paper/7632-deepproblog-neural-probabilistic-logic-programming"><strong>DeepProbLog: Neural Probabilistic Logic Programming</strong></a></p>
<p><em>Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt</em></p>
<p><strong>Abstract:</strong> We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.</p>
<p><a href="http://papers.nips.cc/paper/7669-meta-learning-mcmc-proposals"><strong>Meta-Learning MCMC Proposals</strong></a></p>
<p><em>Tongzhou Wang, Yi Wu, Dave Moore, Stuart J. Russell</em></p>
<p><strong>Abstract:</strong> Effective implementations of sampling-based probabilistic inference often require manually constructed, model-specific proposals. Inspired by recent progresses in meta-learning for training learning agents that can generalize to unseen environments, we propose a meta-learning approach to building effective and generalizable MCMC proposals. We parametrize the proposal as a neural network to provide fast approximations to block Gibbs conditionals. The learned neural proposals generalize to occurrences of common structural motifs across different models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler yields higher final F1 scores than classical single-site Gibbs sampling.</p>
<p><a href="http://papers.nips.cc/paper/7699-importance-weighting-and-variational-inference"><strong>Importance Weighting and Variational Inference</strong></a></p>
<p><em>Justin Domke, Daniel R. Sheldon</em></p>
<p><strong>Abstract:</strong> Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI’s practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.</p>
<p><a href="http://papers.nips.cc/paper/7935-gilbo-one-metric-to-measure-them-all"><strong>GILBO: One Metric to Measure Them All</strong></a></p>
<p><em>Alexander A. Alemi, Ian Fischer</em></p>
<p><strong>Abstract:</strong> We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and discuss the results.</p>
<p><a href="http://papers.nips.cc/paper/8041-graphical-model-inference-sequential-monte-carlo-meets-deterministic-approximations"><strong>Graphical model inference: Sequential Monte Carlo meets deterministic approximations</strong></a></p>
<p><em>Fredrik Lindsten, Jouni Helske, Matti Vihola</em></p>
<p><strong>Abstract:</strong> Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences, but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency, but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically, we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable, we show explicitly how this can be done with loopy belief propagation, expectation propagation, and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and, indeed, numerical results show clear improvements over the baseline deterministic methods as well as over “plain” SMC.</p>
<p><a href="http://papers.nips.cc/paper/8093-a-bayesian-nonparametric-view-on-count-min-sketch"><strong>A Bayesian Nonparametric View on Count-Min Sketch</strong></a></p>
<p><em>Diana Cai, Michael Mitzenmacher, Ryan P. Adams</em></p>
<p><strong>Abstract</strong> The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data with known ground truth, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.</p>
<p><a href="http://papers.nips.cc/paper/8270-autoconj-recognizing-and-exploiting-conjugacy-without-a-domain-specific-language"><strong>Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language</strong></a></p>
<p><em>Matthew D. Hoffman, Matthew J. Johnson, Dustin Tran</em></p>
<p><strong>Abstract</strong> Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies. The package can be downloaded at <a href="https://github.com/google-research/autoconj" class="uri">https://github.com/google-research/autoconj</a>.</p>
<p><a href="http://papers.nips.cc/paper/8015-robust-hypothesis-testing-using-wasserstein-uncertainty-sets"><strong>Robust Hypothesis Testing Using Wasserstein Uncertainty Sets</strong></a></p>
<p><em>Rui Gao, Liyan Xie, Yao Xie, Huan Xu</em></p>
<p><strong>Abstract</strong> We develop a novel computationally efficient and general framework for robust hypothesis testing. The new framework features a new way to construct uncertainty sets under the null and the alternative distributions, which are sets centered around the empirical distribution defined via Wasserstein metric, thus our approach is data-driven and free of distributional assumptions. We develop a convex safe approximation of the minimax formulation and show that such approximation renders a nearly-optimal detector among the family of all possible tests. By exploiting the structure of the least favorable distribution, we also develop a tractable reformulation of such approximation, with complexity independent of the dimension of observation space and can be nearly sample-size-independent in general. Real-data example using human activity data demonstrated the excellent performance of the new robust detector.</p>
<p><a href="http://papers.nips.cc/paper/7304-geometrically-coupled-monte-carlo-sampling"><strong>Geometrically Coupled Monte Carlo Sampling</strong></a></p>
<p><em>Mark Rowland, Krzysztof M. Choromanski, François Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E. Turner, Adrian Weller</em></p>
<p><strong>Abstract</strong> Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks. We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies. We compare our new strategies against prior methods for improving sample efficiency, including QMC, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.</p>
<p><a href="http://papers.nips.cc/paper/7769-assessing-generative-models-via-precision-and-recall"><strong>Assessing Generative Models via Precision and Recall</strong></a></p>
<p><em>Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly</em></p>
<p><strong>Abstract</strong> Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.</p>
<p><a href="http://papers.nips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning"><strong>DAGs with NO TEARS: Continuous Optimization for Structure Learning</strong></a></p>
<p><em>Xun Zheng, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing</em></p>
<p><strong>Abstract</strong> Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.</p>
<p><a href="http://papers.nips.cc/paper/7292-doubly-robust-bayesian-inference-for-non-stationary-streaming-data-with-beta-divergences"><strong>Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with β-Divergences</strong></a></p>
<p><em>Jeremias Knoblauch, Jack E. Jewson, Theodoros Damoulas</em></p>
<p><strong>Abstract</strong> We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with β-divergences. The resulting inference procedure is doubly robust for both the predictive and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as β→0. Secondly, we give a principled way of choosing the divergence parameter β by minimizing expected predictive loss on-line. Reducing False Discovery Rates of from up to 99% to 0% on real world data, this offers the state of the art.</p>
<p><a href="http://papers.nips.cc/paper/8048-the-promises-and-pitfalls-of-stochastic-gradient-langevin-dynamics"><strong>The promises and pitfalls of Stochastic Gradient Langevin Dynamics</strong></a></p>
<p><em>Nicolas Brosse, Alain Durmus, Eric Moulines</em></p>
<p><strong>Abstract</strong> Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated spectacular successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like as Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.</p>
<p><a href="http://papers.nips.cc/paper/7799-reparameterization-gradient-for-non-differentiable-models"><strong>Reparameterization Gradient for Non-differentiable Models</strong></a></p>
<p><em>Wonyeol Lee, Hangyeol Yu, Hongseok Yang</em></p>
<p><strong>Abstract</strong> We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary’s contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.</p>
<p><a href="http://papers.nips.cc/paper/8178-improving-explorability-in-variational-inference-with-annealed-variational-objectives"><strong>Improving Explorability in Variational Inference with Annealed Variational Objectives</strong></a></p>
<p><em>Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, Aaron C. Courville</em></p>
<p><strong>Abstract</strong> Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned. We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods. Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective. In our experiments, we demonstrate our method’s robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space.</p>
<a name="progsynth">
<h3>
Program Synthesis
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/7445-neural-guided-constraint-logic-programming-for-program-synthesis"><strong>Neural Guided Constraint Logic Programming for Program Synthesis</strong></a></p>
<p><em>Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William Byrd, Matthew Might, Raquel Urtasun, Richard Zemel</em></p>
<p><strong>Abstract</strong> Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren’s internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at <a href="https://github.com/xuexue/neuralkanren" class="uri">https://github.com/xuexue/neuralkanren</a>. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.</p>
<p><a href="http://papers.nips.cc/paper/7845-learning-to-infer-graphics-programs-from-hand-drawn-images"><strong>Learning to Infer Graphics Programs from Hand-Drawn Images</strong></a></p>
<p><em>Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, Josh Tenenbaum</em></p>
<p><strong>Abstract</strong> We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of .~The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a specification (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings.</p>
<p><a href="http://papers.nips.cc/paper/8006-learning-libraries-of-subroutines-for-neurallyguided-bayesian-program-induction"><strong>Learning Libraries of Subroutines for Neurally–Guided Bayesian Program Induction</strong></a></p>
<p><em>Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, Josh Tenenbaum</em></p>
<p><strong>Abstract</strong> Successful approaches to program induction require a hand-engineered domain-specific language (DSL), constraining the space of allowed programs and imparting prior knowledge of the domain. We contribute a program induction algorithm that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We use our model to synthesize functions on lists, edit text, and solve symbolic regression problems, showing how the model learns a domain-specific library of program components for expressing solutions to problems in the domain.</p>
<p><a href="http://papers.nips.cc/paper/8086-houdini-lifelong-learning-as-program-synthesis"><strong>HOUDINI: Lifelong Learning as Program Synthesis</strong></a></p>
<p><em>Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, Swarat Chaudhuri</em></p>
<p><strong>Abstract</strong> We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks signiﬁcantly accelerates the search.</p>
<p><a href="http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs"><strong>A Retrieve-and-Edit Framework for Predicting Structured Outputs</strong></a></p>
<p><em>Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy S. Liang</em></p>
<p><strong>Abstract</strong> For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.</p>
<p><a href="http://papers.nips.cc/paper/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics"><strong>Neural Code Comprehension: A Learnable Representation of Code Semantics</strong></a></p>
<p><em>Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler</em></p>
<p><strong>Abstract</strong> With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.</p>
<p><a href="http://papers.nips.cc/paper/7521-tree-to-tree-neural-networks-for-program-translation"><strong>Tree-to-tree Neural Networks for Program Translation</strong></a></p>
<p><em>Xinyun Chen, Chang Liu, Dawn Song</em></p>
<p><strong>Abstract</strong> Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.</p>
<p><a href="http://papers.nips.cc/paper/8107-improving-neural-program-synthesis-with-inferred-execution-traces"><strong>Improving Neural Program Synthesis with Inferred Execution Traces</strong></a></p>
<p><em>Richard Shin, Illia Polosukhin, Dawn Song</em></p>
<p><strong>Abstract</strong> The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, more so than other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces, which contain a superset of the information input/output pairs. While they are highly useful for program synthesis, as execution traces are more difficult to obtain than input/output pairs, we use the insight that we can split the process into two parts: infer the trace from the input/output example, then infer the program from the trace. This simple modification leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work.</p>
<p><a href="http://papers.nips.cc/paper/8204-memory-augmented-policy-optimization-for-program-synthesis-and-semantic-parsing"><strong>Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing</strong></a></p>
<p><em>Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V. Le, Ni Lao</em></p>
<p><strong>Abstract</strong> We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at <a href="https://goo.gl/TXBp4e" class="uri">https://goo.gl/TXBp4e</a></p>
<p><a href="http://papers.nips.cc/paper/7479-automatic-program-synthesis-of-long-programs-with-a-learned-garbage-collector"><strong>Automatic Program Synthesis of Long Programs with a Learned Garbage Collector</strong></a></p>
<p><em>Amit Zohar, Lior Wolf</em></p>
<p><strong>Abstract</strong> We consider the problem of generating automatic code given sample input-output pairs. We train a neural network to map from the current state and the outputs to the program’s next statement. The neural network optimizes multiple tasks concurrently: the next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude. Our code, including an implementation of various literature baselines, is publicly available at <a href="https://github.com/amitz25/PCCoder" class="uri">https://github.com/amitz25/PCCoder</a></p>
<p><a href="http://papers.nips.cc/paper/8001-learning-loop-invariants-for-program-verification"><strong>Learning Loop Invariants for Program Verification</strong></a></p>
<p><em>Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, Le Song</em></p>
<p><strong>Abstract</strong> A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework Code2Inv that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, Code2Inv captures rich program features and avoids the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, it addresses unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate Code2Inv on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 106 problems compared to 73 by a stochastic search-based system, 77 by a heuristic search-based system, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.</p>
<a name="app">
<h3>
Application papers
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/8189-scalable-end-to-end-autonomous-vehicle-testing-via-rare-event-simulation"><strong>Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation</strong></a></p>
<p><em>Matthew O’Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, John C. Duchi</em></p>
<p><strong>Abstract</strong> While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the de facto evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by 2-20 times over naive Monte Carlo sampling methods and 10-300P times (where P is the number of processors) over real-world testing.</p>
<p><a href="http://papers.nips.cc/paper/7877-graph-convolutional-policy-network-for-goal-directed-molecular-graph-generation"><strong>Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation</strong></a></p>
<p><em>Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, Jure Leskovec</em></p>
<p><strong>Abstract</strong> Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models that finds molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.</p>
<p><a href="http://papers.nips.cc/paper/8005-constrained-graph-variational-autoencoders-for-molecule-design"><strong>Constrained Graph Variational Autoencoders for Molecule Design</strong></a></p>
<p><em>Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, Alexander Gaunt</em></p>
<p><strong>Abstract</strong> Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.</p>
<a name="misc">
<h3>
Misc
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations"><strong>Neural Ordinary Differential Equations</strong></a></p>
<p><em>Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, David K. Duvenaud</em></p>
<p><strong>Abstract</strong> We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.</p>
<p><a href="http://papers.nips.cc/paper/7342-a-unified-framework-for-extensive-form-game-abstraction-with-bounds"><strong>A Unified Framework for Extensive-Form Game Abstraction with Bounds</strong></a></p>
<p><em>Christian Kroer, Tuomas Sandholm</em></p>
<p><strong>Abstract</strong> Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this, abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction, and specific solution concepts, for example, exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees—while maintaining comparable bounds on abstraction quality. Moreover, our framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as our results depend on the specific strategy chosen. Nonetheless, we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show, via counterexample, that such assumptions are necessary for some games. Finally, we prove the first bounds for how ϵ-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games.</p>
<p><a href="http://papers.nips.cc/paper/7698-exponentiated-strongly-rayleigh-distributions"><strong>Exponentiated Strongly Rayleigh Distributions</strong></a></p>
<p><em>Zelda E. Mariet, Suvrit Sra, Stefanie Jegelka</em></p>
<p><strong>Abstract</strong> Strongly Rayleigh (SR) measures are discrete probability distributions over the subsets of a ground set. They enjoy strong negative dependence properties, as a result of which they assign higher probability to subsets of diverse elements. We introduce in this paper Exponentiated Strongly Rayleigh (ESR) measures, which sharpen (or smoothen) the negative dependence property of SR measures via a single parameter (the exponent) that can intuitively understood as an inverse temperature. We develop efficient MCMC procedures for approximate sampling from ESRs, and obtain explicit mixing time bounds for two concrete instances: exponentiated versions of Determinantal Point Processes and Dual Volume Sampling. We illustrate some of the potential of ESRs, by applying them to a few machine learning tasks; empirical results confirm that beyond their theoretical appeal, ESR-based models hold significant promise for these tasks.</p>
<p><a href="http://papers.nips.cc/paper/7599-learning-to-optimize-tensor-programs"><strong>Learning to Optimize Tensor Programs</strong></a></p>
<p><em>Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy</em></p>
<p><strong>Abstract</strong> We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.</p>
<p><a href="http://papers.nips.cc/paper/7502-modelling-sparsity-heterogeneity-reciprocity-and-community-structure-in-temporal-interaction-data"><strong>Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data</strong></a></p>
<p><em>Xenia Miscouridou, Francois Caron, Yee Whye Teh</em></p>
<p><strong>Abstract</strong> We propose a novel class of network models for temporal dyadic interaction data. Our objective is to capture important features often observed in social interactions: sparsity, degree heterogeneity, community structure and reciprocity. We use mutually-exciting Hawkes processes to model the interactions between each (directed) pair of individuals. The intensity of each process allows interactions to arise as responses to opposite interactions (reciprocity), or due to shared interests between individuals (community structure). For sparsity and degree heterogeneity, we build the non time dependent part of the intensity function on compound random measures following Todeschini et al., 2016. We conduct experiments on real-world temporal interaction data and show that the proposed model outperforms competing approaches for link prediction, and leads to interpretable parameters.</p>
<h2 id="workshop-papers">Workshop papers:</h2>
<p>Infer to Control workshop</p>
<p><a href="https://arxiv.org/abs/1811.01132"><strong>VIREL: A Variational Inference Framework for Reinforcement Learning</strong></a></p>
<p><em>Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner, Shimon Whiteson</em></p>
<p><strong>Abstract</strong> Applying probabilistic models to reinforcement learning (RL) has become an exciting direction of research owing to powerful optimisation tools such as variational inference becoming applicable to RL. However, due to their formulation, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, for example, the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties in optimisation of learning objective in maximum entropy RL based approaches. We propose VIREL, a novel, theoretically grounded probabilistic inference framework for RL that utilises the action-value function in a parametrised form to capture future dynamics of the underlying Markov decision process. Owing to its generality, our framework lends itself to current advances in variational inference. Applying the variational expectation-maximisation algorithm to our framework, we show that the actor-critic algorithm can be reduced to expectation-maximisation. We derive a family of methods from our framework, including state-of-the-art methods based on soft value functions. We evaluate two actor-critic algorithms derived from this family, which perform on par with soft actor critic, demonstrating that our framework offers a promising perspective on RL as inference.</p>

</div>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Fri, 21 Dec 2018 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2018-12-21-nips-reviews/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Decomposing the ELBO</title>
    <link>http://zinkov.com/posts/2018-11-02-decomposing-the-elbo/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Decomposing the ELBO</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Decomposing the ELBO</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2018-11-02</h5>
</div>

<div class="text-left">

<p>When performing Variational Inference, we are minimizing the KL divergence between some distribution we care about <span class="math inline">\(p(\v{z} \mid \v{x})\)</span> and some distribution that is easier to work with <span class="math inline">\(q_\phi(\v{z} \mid \v{x})\)</span>.</p>
<p><span class="math display">\[
\begin{align}
	\phi^* &amp;= \underset{\phi}{\mathrm{argmin}}\, \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z} \mid \v{x})) \\
		&amp;= \underset{\phi}{\mathrm{argmin}}\,
  \mathbb{E}_{q_\phi(\v{z} \mid \v{x})}
  \big[\log q_\phi(\v{z} \mid \v{x})
  -
  \log p(\v{z} \mid \v{x})
  \big]\\
\end{align}
\]</span></p>
<p>Now because the density of <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span> usually isn’t tractable, we use a property of the log model evidence <span class="math inline">\(\log\, p(\v{x})\)</span> to define a different objective to optimize.</p>
<p><span class="math display">\[
\begin{align}
\Expect_{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{z} \mid \v{x})\big]
&amp;\leq \Expect_{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{z} \mid \v{x})\big] - \log p(\v{x}) \\
&amp;= \Expect _{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{z} \mid \v{x}) - \log p(\v{x})\big] \\
&amp;= \Expect _{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{x}, \v{z})\big]\\
&amp;= -\mathcal{L}(\phi)
\end{align}
\]</span></p>
<p>As <span class="math inline">\(\mathcal{L}(\phi) = \log p(\v{x}) - \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z} \mid \v{x}))\)</span> maximizing <span class="math inline">\(\mathcal{L}(\phi)\)</span> effectively minimizes our original KL.</p>
<p>This term <span class="math inline">\(\mathcal{L}(\phi)\)</span> is sometimes called the evidence lower-bound or ELBO, because the KL term must always be greater-than or equal to zero, <span class="math inline">\(\mathcal{L}(\phi)\)</span> can be seen as a lower-bound estimate of <span class="math inline">\(\log p(\v{x})\)</span>.</p>
<p>Due to various linearity properties of expectations, this can be rearranged into many different forms. This is useful to get an intuition for what can be going wrong when you learn <span class="math inline">\(q_\phi(\v{z} \mid \v{x})\)</span></p>
<p>Now why does this matter? Couldn’t I just optimize this loss with SGD and be done? Well you can, but often if something is going wrong it will show up as one or some terms being unusually off. By making these tradeoffs in the loss function explicit means you can adjust it to favor different properties of your learned representation. Either by hand or automatically.</p>
<h2 id="entropy-form">Entropy form</h2>
<p>The classic form is in terms of an energy term and an entropy term. The first term encourages <span class="math inline">\(q\)</span> to put high probability mass wherever <span class="math inline">\(p\)</span> does so. The second term is encouraging that <span class="math inline">\(q\)</span> should as much as possible maximize it’s entropy and put probability mass everywhere it can.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}[\log p(x, z)] + H(q_\phi(\v{z} \mid \v{x})) \]</span></p>
<p>where</p>
<p><span class="math display">\[ H(q_\phi(\v{z} \mid \v{x})) \triangleq - \Expect_{q_\phi(\v{z} \mid \v{x})}[\log q_\phi(\v{z} \mid \v{x})] \]</span></p>
<h2 id="reconstruction-error-minus-kl-on-the-prior">Reconstruction error minus KL on the prior</h2>
<p>More often these days, we describe the <span class="math inline">\(\mathcal{L}\)</span> in terms of a reconstruction term and KL on the prior for <span class="math inline">\(p\)</span>. Here the first term is saying we should put mass on latent codes <span class="math inline">\(\v{z}\)</span> from which <span class="math inline">\(p\)</span> is likely to generate our observation <span class="math inline">\(\v{x}\)</span>. The second term then suggests to this trade off with <span class="math inline">\(q\)</span> also being near the prior.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}[\log p(\v{x} \mid \v{z})] - \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z}))\]</span></p>
<h2 id="elbo-surgery">ELBO surgery</h2>
<p>But there are other ways to think about this decomposition. Because we frequently use amortized inference to learn a <span class="math inline">\(\phi\)</span> useful for describing all kinds of <span class="math inline">\(q\)</span> distributions regardless of our choice of observation <span class="math inline">\(\v{x}\)</span>. We can talk about the average distribution we learn over our observed data, with <span class="math inline">\(p_d\)</span> being the empirical distribution of our observations.</p>
<p><span class="math display">\[ \overline{q}_\phi(\v{z}) = \Expect_{p_d(\v{x})} \big[ q_\phi(\v{z} \mid \v{x}) \big] \]</span></p>
<p>This is sometimes called the aggregate posterior.</p>
<p>With that we can decompose our KL on the prior into a mutual information term that encourages each <span class="math inline">\(q_\phi(\v{z} \mid \v{x})\)</span> we create to be near the average one <span class="math inline">\(\overline{q}_\phi(\v{z})\)</span> and a KL between this average distribution and the prior. The encourages the representation generated for <span class="math inline">\(\v{z}\)</span> to be useful.</p>
<p><span class="math display">\[ w\mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}[\log p(\v{x} \mid \v{z})] - \mathbb{I}_q(\v{x},\v{z})  - \text{KL}(\overline{q}_\phi(\v{z}) \;\|\; p(\v{z})) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \mathbb{I}_q(\v{x},\v{z}) \triangleq \Expect_{p_d}\big[\Expect_{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x})\big] \big]
- \Expect_{\overline{q}_\phi(\v{z})} \log \overline{q}_\phi(\v{z}) \]</span></p>
<h2 id="difference-of-two-kl-divergences">Difference of two KL divergences</h2>
<p>With something like <span class="math inline">\(p_d\)</span> around it is also possible to pull out the relationship between <span class="math inline">\(p\)</span> and <span class="math inline">\(p_d\)</span>. This is particularly relevant if you intend to learn <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = - \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z} \mid \v{x})) - \text{KL}(p_d(\v{x}) \;\|\; p(\v{x})) \]</span></p>
<h2 id="full-decomposition">Full decomposition</h2>
<p>Of course with more aggressive rearranging, we can just have a term to encourage learning better latent representations. In a setting where you aren’t learning <span class="math inline">\(p\)</span> some of these terms are constant and can generally be ignored. I provide them here for completeness.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}\left[ \log\frac{p(\v{x} \mid \v{z})}{p(\v{x})}
- \log\frac{q_\phi(\v{z} \mid \v{x})}{q_\phi(\v{z})} \right]
- \text{KL}(p_d(\v{x}) \;\|\; p(\v{x}))
- \text{KL}(\overline{q}_\phi(\v{z}) \;\|\; p(\v{z}))\]</span></p>
<p>I highly encourage checking out the Appendix of the Structured Disentangled Representations paper to see how much further this can be pushed.</p>
<h2 id="final-notes">Final notes</h2>
<p>Of course, all the above still holds in the VAE setting where <span class="math inline">\(p\)</span> becomes <span class="math inline">\(p_\theta\)</span> but I felt the notation was cluttered enough already. It’s kind of amazing how much insight can be gained through expanding and collapsing one loss function.</p>
<h2 id="further-references">Further references</h2>
<ul>
<li><a href="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf">ELBO Surgery: yet another way to carve up the variational evidence lower bound</a></li>
<li><a href="https://arxiv.org/pdf/1711.00464.pdf">Fixing a Broken ELBO</a></li>
<li><a href="https://arxiv.org/pdf/1804.02086.pdf">Structured Disentangled Representations</a></li>
<li><a href="https://arxiv.org/abs/1511.06349">Generating Sentences from a Continuous Space</a></li>
<li><a href="https://arxiv.org/abs/1702.08658">Towards Deeper Understanding of Variational Autoencoding Models</a></li>
</ul>

</div>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Fri, 02 Nov 2018 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2018-11-02-decomposing-the-elbo/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Calculating the golden-era of The Simpsons</title>
    <link>http://zinkov.com/posts/2017-11-03-simpsons-changepoint/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Calculating the golden-era of The Simpsons</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Calculating the golden-era of The Simpsons</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2017-11-03</h5>
</div>

<div class="text-left">

<p><a href="http://www.nathancunn.com/">Nathan Cunningham</a> published last week a <a href="http://www.nathancunn.com/2017-10-26-simpsons-decline/">fantastic</a> article about using some stats to estimate at what episode did Simpsons start to decline.</p>
<p>Cameron Davidson-Pilon suggested this would make a great application of Bayesian changepoint models.</p>
<blockquote class="twitter-tweet" data-lang="en-gb">
<p lang="en" dir="ltr">
Someone want to Bayesian switchpoint model this? See first chapter of BMH <a href="https://t.co/QiGwzA0khD">https://t.co/QiGwzA0khD</a>
</p>
— Cam DP 👨🏽‍💻 (<span class="citation" data-cites="Cmrn_DP">@Cmrn_DP</span>) <a href="https://twitter.com/Cmrn_DP/status/924360674041585664?ref_src=twsrc%5Etfw">28 October 2017</a>
</blockquote>
<p>In turns out, he was totally right. Taking the <a href="http://docs.pymc.io/notebooks/getting_started.html#Case-study-2:-Coal-mining-disasters">Coal-mining disaster</a> example from the pymc3 quickstart guide and slightly modifying it is enough to do the job.</p>
<p>First we load the data</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">data <span class="op">=</span> pd.read_csv(<span class="st">&quot;simpsons_ratings.csv&quot;</span>)</a>
<a class="sourceLine" id="cb1-2" title="2">index <span class="op">=</span> data.index</a></code></pre></div>
<p>Then we use some Gaussians to describe the average rating, and how that mean rate translates to the quality of any particular episode.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</a>
<a class="sourceLine" id="cb2-2" title="2">    switch <span class="op">=</span> pm.DiscreteUniform(<span class="st">'switch'</span>, lower<span class="op">=</span>index.<span class="bu">min</span>(), upper<span class="op">=</span>index.<span class="bu">max</span>())</a>
<a class="sourceLine" id="cb2-3" title="3">    early_mean <span class="op">=</span> pm.Normal(<span class="st">'early_mean'</span>, mu<span class="op">=</span><span class="fl">5.</span>, sd<span class="op">=</span><span class="fl">1.</span>)</a>
<a class="sourceLine" id="cb2-4" title="4">    late_mean <span class="op">=</span> pm.Normal(<span class="st">'late_mean'</span>, mu<span class="op">=</span><span class="fl">5.</span>, sd<span class="op">=</span><span class="fl">1.</span>)</a>
<a class="sourceLine" id="cb2-5" title="5">    mean <span class="op">=</span> tt.switch(switch <span class="op">&gt;=</span> index.values, early_mean, late_mean)</a>
<a class="sourceLine" id="cb2-6" title="6">    ratings <span class="op">=</span> pm.Normal(<span class="st">'ratings'</span>, mu<span class="op">=</span>mean, sd<span class="op">=</span><span class="fl">1.</span>,</a>
<a class="sourceLine" id="cb2-7" title="7">                        observed<span class="op">=</span>data[<span class="st">&quot;UserRating&quot;</span>].values)</a>
<a class="sourceLine" id="cb2-8" title="8"></a>
<a class="sourceLine" id="cb2-9" title="9">    tr <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">500</span>)</a>
<a class="sourceLine" id="cb2-10" title="10">    pm.traceplot(tr)</a></code></pre></div>
<p><img src="../../images/simpsons_trace.png" /></p>
<p>As we can see around 220 is when our model thinks the Simpsons was starting to downward slide.</p>
<p>That would be</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(data[<span class="st">&quot;EpisodeID&quot;</span>][<span class="dv">220</span>], data[<span class="st">&quot;Episode&quot;</span>][<span class="dv">220</span>]))</a>
<a class="sourceLine" id="cb3-2" title="2"><span class="co"># &gt;&gt;&gt; S10E18: Simpsons Bible Stories</span></a></code></pre></div>
<p>An episode I remember being <em>alright</em>. Generally the 10th season is acknowledged as the last of the golden years. In fact, <a href="https://twitter.com/woohootriviachi">Chicago Simpsons Trivia Night</a> bills itself as not asking any questions from seasons after 10.</p>
<p>Apologies in advance for not using more Simpsons jokes in this post. You can find the code and data I used on <a href="https://github.com/zaxtax/simpsons_changepoint">github</a>.</p>

</div>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Fri, 03 Nov 2017 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2017-11-03-simpsons-changepoint/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Indentation sensitive parsing the easy way</title>
    <link>http://zinkov.com/posts/2016-01-12-indentation-sensitive-parsing/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Indentation sensitive parsing the easy way</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Indentation sensitive parsing the easy way</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2016-01-12</h5>
</div>

<div class="text-left">

<p>Recently, I had to write an parser for <a href="https://github.com/hakaru-dev/hakaru">Hakaru</a>. Writing parsers in Haskell is generally a treat as there are muliple parser libraries to choose from including Happy, parsec, attoparsec, megaparsec, trifecta, and many others. The trouble occurs when you want to parse and indentation-sensitive language like Python or Haskell. For that task the choices are more limited and far less documented. Which is unfortunate as my favorite library <a href="https://hackage.haskell.org/package/indentation">indentation</a> of the bunch is the least documented. The following is how to use <code>indentation</code> to write an indentation-sensitive parser.</p>
<p>For this tutorial, I will use <code>indentation</code> and <code>parsec</code>.</p>
<pre><code>cabal install indentation parsec</code></pre>
<p>To get started import Parsec as you normally would</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" title="1"><span class="kw">module</span> <span class="dt">Indent.Demo</span> <span class="kw">where</span></a>
<a class="sourceLine" id="cb2-2" title="2"></a>
<a class="sourceLine" id="cb2-3" title="3"><span class="kw">import</span>           <span class="dt">Data.Functor</span>                  ((&lt;$&gt;), (&lt;$))</a>
<a class="sourceLine" id="cb2-4" title="4"><span class="kw">import</span>           <span class="dt">Control.Applicative</span>           (<span class="dt">Applicative</span>(..))</a>
<a class="sourceLine" id="cb2-5" title="5"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Control.Monad</span>                 <span class="kw">as</span> <span class="dt">M</span></a>
<a class="sourceLine" id="cb2-6" title="6"><span class="kw">import</span>           <span class="dt">Data.Functor.Identity</span></a>
<a class="sourceLine" id="cb2-7" title="7"><span class="kw">import</span>           <span class="dt">Data.Text</span>                     (<span class="dt">Text</span>)</a>
<a class="sourceLine" id="cb2-8" title="8"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Data.Text</span>                     <span class="kw">as</span> <span class="dt">Text</span></a>
<a class="sourceLine" id="cb2-9" title="9"><span class="kw">import</span>           <span class="dt">Text.Parsec</span>                   <span class="kw">hiding</span> (<span class="dt">Empty</span>)</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="kw">import</span>           <span class="dt">Text.Parsec.Text</span>              () <span class="co">-- instances only</span></a>
<a class="sourceLine" id="cb2-11" title="11"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Text.Parsec.Expr</span>              <span class="kw">as</span> <span class="dt">Ex</span></a>
<a class="sourceLine" id="cb2-12" title="12"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Text.Parsec.Token</span>             <span class="kw">as</span> <span class="dt">Tok</span></a></code></pre></div>
<p>And then add the following modules from <code>indentation</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">import</span>           <span class="dt">Text.Parsec.Indentation</span></a>
<a class="sourceLine" id="cb3-2" title="2"><span class="kw">import</span>           <span class="dt">Text.Parsec.Indentation.Char</span></a>
<a class="sourceLine" id="cb3-3" title="3"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Text.Parsec.Indentation.Token</span> <span class="kw">as</span> <span class="dt">ITok</span></a></code></pre></div>
<p>The key thing which needs to be changed is that the lexer needs to be indentation-sensitive. Sadly, there is no easy way to extend the existing LanguageDefs, so we make one from scratch.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" title="1"><span class="ot">style ::</span> <span class="dt">Tok.GenLanguageDef</span> <span class="dt">ParserStream</span> st <span class="dt">Identity</span></a>
<a class="sourceLine" id="cb4-2" title="2">style <span class="ot">=</span> ITok.makeIndentLanguageDef <span class="op">$</span> <span class="dt">Tok.LanguageDef</span></a>
<a class="sourceLine" id="cb4-3" title="3">    { Tok.commentStart    <span class="ot">=</span> <span class="st">&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-4" title="4">    , Tok.commentEnd      <span class="ot">=</span> <span class="st">&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-5" title="5">    , Tok.nestedComments  <span class="ot">=</span> <span class="dt">True</span></a>
<a class="sourceLine" id="cb4-6" title="6">    , Tok.identStart      <span class="ot">=</span> letter <span class="op">&lt;|&gt;</span> char <span class="ch">'_'</span></a>
<a class="sourceLine" id="cb4-7" title="7">    , Tok.identLetter     <span class="ot">=</span> alphaNum <span class="op">&lt;|&gt;</span> oneOf <span class="st">&quot;_'&quot;</span></a>
<a class="sourceLine" id="cb4-8" title="8">    , Tok.opStart         <span class="ot">=</span> oneOf <span class="st">&quot;!#$%&amp;*+./&lt;=&gt;?@\\^|-~&quot;</span></a>
<a class="sourceLine" id="cb4-9" title="9">    , Tok.opLetter        <span class="ot">=</span> oneOf <span class="st">&quot;!#$%&amp;*+./&lt;=&gt;?@\\^|-~&quot;</span></a>
<a class="sourceLine" id="cb4-10" title="10">    , Tok.caseSensitive   <span class="ot">=</span> <span class="dt">True</span></a>
<a class="sourceLine" id="cb4-11" title="11">    , Tok.commentLine     <span class="ot">=</span> <span class="st">&quot;#&quot;</span></a>
<a class="sourceLine" id="cb4-12" title="12">    , Tok.reservedOpNames <span class="ot">=</span> [<span class="st">&quot;:&quot;</span>]</a>
<a class="sourceLine" id="cb4-13" title="13">    , Tok.reservedNames   <span class="ot">=</span> [<span class="st">&quot;def&quot;</span>, <span class="st">&quot;add&quot;</span>]</a>
<a class="sourceLine" id="cb4-14" title="14">    }</a>
<a class="sourceLine" id="cb4-15" title="15"></a>
<a class="sourceLine" id="cb4-16" title="16"><span class="ot">lexer ::</span> <span class="dt">Tok.GenTokenParser</span> <span class="dt">ParserStream</span> () <span class="dt">Identity</span></a>
<a class="sourceLine" id="cb4-17" title="17">lexer <span class="ot">=</span> ITok.makeTokenParser style</a></code></pre></div>
<p>Once you have an indentation-sensitive lexer, you can add the primitives you need in terms of it.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" title="1"><span class="ot">integer ::</span> <span class="dt">Parser</span> <span class="dt">Integer</span></a>
<a class="sourceLine" id="cb5-2" title="2">integer <span class="ot">=</span> Tok.integer lexer</a>
<a class="sourceLine" id="cb5-3" title="3"></a>
<a class="sourceLine" id="cb5-4" title="4"><span class="ot">identifier ::</span> <span class="dt">Parser</span> <span class="dt">String</span></a>
<a class="sourceLine" id="cb5-5" title="5">identifier <span class="ot">=</span> Tok.identifier lexer</a>
<a class="sourceLine" id="cb5-6" title="6"></a>
<a class="sourceLine" id="cb5-7" title="7"><span class="ot">reserved ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Parser</span> ()</a>
<a class="sourceLine" id="cb5-8" title="8">reserved <span class="ot">=</span> Tok.reserved lexer</a>
<a class="sourceLine" id="cb5-9" title="9"></a>
<a class="sourceLine" id="cb5-10" title="10"><span class="ot">reservedOp ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Parser</span> ()</a>
<a class="sourceLine" id="cb5-11" title="11">reservedOp <span class="ot">=</span> Tok.reservedOp lexer</a>
<a class="sourceLine" id="cb5-12" title="12"></a>
<a class="sourceLine" id="cb5-13" title="13"><span class="ot">parens ::</span> <span class="dt">Parser</span> a <span class="ot">-&gt;</span> <span class="dt">Parser</span> a</a>
<a class="sourceLine" id="cb5-14" title="14">parens <span class="ot">=</span> Tok.parens lexer <span class="op">.</span> localIndentation <span class="dt">Any</span></a>
<a class="sourceLine" id="cb5-15" title="15"></a>
<a class="sourceLine" id="cb5-16" title="16"><span class="ot">commaSep ::</span> <span class="dt">Parser</span> a <span class="ot">-&gt;</span> <span class="dt">Parser</span> [a]</a>
<a class="sourceLine" id="cb5-17" title="17">commaSep <span class="ot">=</span> Tok.commaSep lexer</a></code></pre></div>
<p>All of these are boilerplate except for <code>parens</code>. You will notice, for it we call <code>localIndentation Any</code> before passing the input. This function indicates that indentation rules can be ignored when using this combinator. This gives parentheses the meaning they have in python which is to suspend indentation rules. We will go into more detail how the indentation primitives work, but for now let’s define AST for our language.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">type</span> <span class="dt">Name</span> <span class="ot">=</span> <span class="dt">String</span></a>
<a class="sourceLine" id="cb6-2" title="2"><span class="kw">type</span> <span class="dt">Args</span> <span class="ot">=</span> [<span class="dt">Name</span>]</a>
<a class="sourceLine" id="cb6-3" title="3"></a>
<a class="sourceLine" id="cb6-4" title="4"><span class="kw">type</span> <span class="dt">ParserStream</span>    <span class="ot">=</span> <span class="dt">IndentStream</span> (<span class="dt">CharIndentStream</span> <span class="dt">String</span>)</a>
<a class="sourceLine" id="cb6-5" title="5"><span class="kw">type</span> <span class="dt">Parser</span>          <span class="ot">=</span> <span class="dt">ParsecT</span>     <span class="dt">ParserStream</span> () <span class="dt">Identity</span></a>
<a class="sourceLine" id="cb6-6" title="6"></a>
<a class="sourceLine" id="cb6-7" title="7"><span class="kw">data</span> <span class="dt">Expr</span> <span class="ot">=</span></a>
<a class="sourceLine" id="cb6-8" title="8">     <span class="dt">Func</span> <span class="dt">Name</span> <span class="dt">Args</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb6-9" title="9">   <span class="op">|</span> <span class="dt">Var</span>  <span class="dt">Name</span></a>
<a class="sourceLine" id="cb6-10" title="10">   <span class="op">|</span> <span class="dt">App</span>  <span class="dt">Expr</span> [<span class="dt">Expr</span>]</a>
<a class="sourceLine" id="cb6-11" title="11">   <span class="op">|</span> <span class="dt">Add</span>  <span class="dt">Expr</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb6-12" title="12">   <span class="op">|</span> <span class="dt">Lit</span>  <span class="dt">Integer</span></a>
<a class="sourceLine" id="cb6-13" title="13">   <span class="kw">deriving</span> (<span class="dt">Show</span>)</a></code></pre></div>
<p>Parsing this language doesn’t involve need to involve indentation rules</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" title="1"><span class="ot">int ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-2" title="2">int <span class="ot">=</span> <span class="dt">Lit</span> <span class="op">&lt;$&gt;</span> integer</a>
<a class="sourceLine" id="cb7-3" title="3"></a>
<a class="sourceLine" id="cb7-4" title="4"><span class="ot">add ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-5" title="5">add <span class="ot">=</span> reserved <span class="st">&quot;add&quot;</span> <span class="op">*&gt;</span> (<span class="dt">Add</span> <span class="op">&lt;$&gt;</span> expr <span class="op">&lt;*&gt;</span> expr)</a>
<a class="sourceLine" id="cb7-6" title="6"></a>
<a class="sourceLine" id="cb7-7" title="7"><span class="ot">var ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-8" title="8">var <span class="ot">=</span> <span class="dt">Var</span> <span class="op">&lt;$&gt;</span> identifier</a>
<a class="sourceLine" id="cb7-9" title="9"></a>
<a class="sourceLine" id="cb7-10" title="10"><span class="ot">app ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-11" title="11">app <span class="ot">=</span> <span class="dt">App</span> <span class="op">&lt;$&gt;</span> var <span class="op">&lt;*&gt;</span> parens (commaSep expr)</a>
<a class="sourceLine" id="cb7-12" title="12"></a>
<a class="sourceLine" id="cb7-13" title="13"><span class="ot">def ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-14" title="14">def <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb7-15" title="15">  reserved <span class="st">&quot;def&quot;</span></a>
<a class="sourceLine" id="cb7-16" title="16">  name <span class="ot">&lt;-</span> identifier</a>
<a class="sourceLine" id="cb7-17" title="17">  args <span class="ot">&lt;-</span> parens (commaSep identifier)</a>
<a class="sourceLine" id="cb7-18" title="18">  body <span class="ot">&lt;-</span> reservedOp <span class="st">&quot;:&quot;</span> <span class="op">*&gt;</span> expr</a>
<a class="sourceLine" id="cb7-19" title="19">  <span class="fu">return</span> (<span class="dt">Func</span> name args body)</a>
<a class="sourceLine" id="cb7-20" title="20"></a>
<a class="sourceLine" id="cb7-21" title="21"><span class="ot">expr ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-22" title="22">expr <span class="ot">=</span> def</a>
<a class="sourceLine" id="cb7-23" title="23">   <span class="op">&lt;|&gt;</span> try app</a>
<a class="sourceLine" id="cb7-24" title="24">   <span class="op">&lt;|&gt;</span> try var</a>
<a class="sourceLine" id="cb7-25" title="25">   <span class="op">&lt;|&gt;</span> try add</a>
<a class="sourceLine" id="cb7-26" title="26">   <span class="op">&lt;|&gt;</span> int</a>
<a class="sourceLine" id="cb7-27" title="27">   <span class="op">&lt;|&gt;</span> parens expr</a></code></pre></div>
<p>Let’s add some helper code.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb8-1" title="1"><span class="ot">indentConfig ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">ParserStream</span></a>
<a class="sourceLine" id="cb8-2" title="2">indentConfig <span class="ot">=</span></a>
<a class="sourceLine" id="cb8-3" title="3">    mkIndentStream <span class="dv">0</span> infIndentation <span class="dt">True</span> <span class="dt">Ge</span> <span class="op">.</span> mkCharIndentStream</a>
<a class="sourceLine" id="cb8-4" title="4"></a>
<a class="sourceLine" id="cb8-5" title="5"><span class="ot">parse ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Either</span> <span class="dt">ParseError</span> [<span class="dt">Expr</span>]</a>
<a class="sourceLine" id="cb8-6" title="6">parse <span class="ot">=</span></a>
<a class="sourceLine" id="cb8-7" title="7">    runParser (many expr <span class="op">&lt;*</span> eof) () <span class="st">&quot;[input]&quot;</span> <span class="op">.</span> indentConfig</a></code></pre></div>
<p>And this parses programs just fine.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb9-1" title="1">test1 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb9-2" title="2">  [ <span class="st">&quot;def foo(x,y):&quot;</span></a>
<a class="sourceLine" id="cb9-3" title="3">  , <span class="st">&quot;    add x y&quot;</span></a>
<a class="sourceLine" id="cb9-4" title="4">  ]</a>
<a class="sourceLine" id="cb9-5" title="5"></a>
<a class="sourceLine" id="cb9-6" title="6">parse test1</a>
<a class="sourceLine" id="cb9-7" title="7"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>The issue is also things which feel invalid.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb10-1" title="1">test2 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb10-2" title="2">  [ <span class="st">&quot;def foo(x,y):&quot;</span></a>
<a class="sourceLine" id="cb10-3" title="3">  , <span class="st">&quot;add x y&quot;</span></a>
<a class="sourceLine" id="cb10-4" title="4">  ]</a>
<a class="sourceLine" id="cb10-5" title="5"></a>
<a class="sourceLine" id="cb10-6" title="6">parse test2</a>
<a class="sourceLine" id="cb10-7" title="7"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>We need to change <code>def</code> so that its body must be indented at strictly greater than character where it starts.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb11-1" title="1"><span class="ot">blockExpr ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb11-2" title="2">blockExpr <span class="ot">=</span> reservedOp <span class="st">&quot;:&quot;</span> <span class="op">*&gt;</span> localIndentation <span class="dt">Gt</span> expr</a>
<a class="sourceLine" id="cb11-3" title="3"></a>
<a class="sourceLine" id="cb11-4" title="4"><span class="ot">def ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb11-5" title="5">def <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb11-6" title="6">  reserved <span class="st">&quot;def&quot;</span></a>
<a class="sourceLine" id="cb11-7" title="7">  name <span class="ot">&lt;-</span> identifier</a>
<a class="sourceLine" id="cb11-8" title="8">  args <span class="ot">&lt;-</span> parens (commaSep identifier)</a>
<a class="sourceLine" id="cb11-9" title="9">  body <span class="ot">&lt;-</span> blockExpr</a>
<a class="sourceLine" id="cb11-10" title="10">  <span class="fu">return</span> (<span class="dt">Func</span> name args body)</a></code></pre></div>
<p>If you now look, we have defined a function for the body, <code>blockExpr</code>, which says we must have the body strictly greater. Now when we parse <code>test2</code> we get the following.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb12-1" title="1">parse test2</a>
<a class="sourceLine" id="cb12-2" title="2"><span class="co">-- Left &quot;[input]&quot; (line 2, column 2):</span></a>
<a class="sourceLine" id="cb12-3" title="3"><span class="co">-- expecting identifier</span></a>
<a class="sourceLine" id="cb12-4" title="4"><span class="co">--</span></a>
<a class="sourceLine" id="cb12-5" title="5"><span class="co">-- Invalid indentation.</span></a>
<a class="sourceLine" id="cb12-6" title="6"><span class="co">--   Found a token at indentation 1.</span></a>
<a class="sourceLine" id="cb12-7" title="7"><span class="co">--   Expecting a token at an indentation greater than or equal to 2.</span></a>
<a class="sourceLine" id="cb12-8" title="8"><span class="co">--   IndentStream { indentationState =</span></a>
<a class="sourceLine" id="cb12-9" title="9"><span class="co">--                   IndentationState { minIndentation = 2</span></a>
<a class="sourceLine" id="cb12-10" title="10"><span class="co">--                                    , maxIndentation = 9223372036854775807</span></a>
<a class="sourceLine" id="cb12-11" title="11"><span class="co">--                                    , absMode = False</span></a>
<a class="sourceLine" id="cb12-12" title="12"><span class="co">--                                    , tokenRel = Ge}</span></a>
<a class="sourceLine" id="cb12-13" title="13"><span class="co">--                , tokenStream = &quot;&quot;}</span></a></code></pre></div>
<p><code>localIndentation</code> takes two arguments, what the indentation of an expression should be relative to the current indentation, and the expression itself. Relative indentations can be greater-than and equal (Ge), strictly greater-than (Gt), equal (Eq), a specific amount (Const 5), or anything (Any).</p>
<p>While it seems like this is the only primitive you should need, sometimes the indentation level you want can’t be defined in terms of the parent.</p>
<p>For example, the following is a valid program</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb13-1" title="1">test3 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb13-2" title="2">  [ <span class="st">&quot;def foo(x, y):&quot;</span></a>
<a class="sourceLine" id="cb13-3" title="3">  , <span class="st">&quot;    add x&quot;</span></a>
<a class="sourceLine" id="cb13-4" title="4">  , <span class="st">&quot; y&quot;</span></a>
<a class="sourceLine" id="cb13-5" title="5">  ]</a>
<a class="sourceLine" id="cb13-6" title="6"></a>
<a class="sourceLine" id="cb13-7" title="7">parse test4</a>
<a class="sourceLine" id="cb13-8" title="8"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>The issue is that “y” is indented greater than the “def” but, we really want it to be indented in terms of “add”. To do this we need to use absolute indentation. This mode says indentation is defined in terms of the first token parsed, and all indentation rules apply in terms of where that first token is found.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb14-1" title="1"><span class="ot">absBlockExpr ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb14-2" title="2">absBlockExpr <span class="ot">=</span> reservedOp <span class="st">&quot;:&quot;</span> <span class="op">*&gt;</span> localIndentation <span class="dt">Gt</span> (absoluteIndentation expr)</a>
<a class="sourceLine" id="cb14-3" title="3"></a>
<a class="sourceLine" id="cb14-4" title="4"></a>
<a class="sourceLine" id="cb14-5" title="5"><span class="ot">def ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb14-6" title="6">def <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb14-7" title="7">  reserved <span class="st">&quot;def&quot;</span></a>
<a class="sourceLine" id="cb14-8" title="8">  name <span class="ot">&lt;-</span> identifier</a>
<a class="sourceLine" id="cb14-9" title="9">  args <span class="ot">&lt;-</span> parens (commaSep identifier)</a>
<a class="sourceLine" id="cb14-10" title="10">  body <span class="ot">&lt;-</span> absBlockExpr</a>
<a class="sourceLine" id="cb14-11" title="11">  <span class="fu">return</span> (<span class="dt">Func</span> name args body)</a></code></pre></div>
<p>We define a function absBlockExpr. You’ll notice we also used a <code>localIndentation</code>. The reason for that is <code>absolutionIndentation</code> normally defaults to the first token of the parent. In our case, this is <code>def</code> and we want instead for it to choose <code>add</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb15-1" title="1">parse test3</a>
<a class="sourceLine" id="cb15-2" title="2"><span class="co">-- Left &quot;[input]&quot; (line 3, column 3):</span></a>
<a class="sourceLine" id="cb15-3" title="3"><span class="co">-- expecting identifier</span></a>
<a class="sourceLine" id="cb15-4" title="4"><span class="co">--</span></a>
<a class="sourceLine" id="cb15-5" title="5"><span class="co">-- Invalid indentation.</span></a>
<a class="sourceLine" id="cb15-6" title="6"><span class="co">--   Found a token at indentation 2.</span></a>
<a class="sourceLine" id="cb15-7" title="7"><span class="co">--   Expecting a token at an indentation greater than or equal to 5.</span></a>
<a class="sourceLine" id="cb15-8" title="8"><span class="co">--   IndentStream { indentationState =</span></a>
<a class="sourceLine" id="cb15-9" title="9"><span class="co">--                   IndentationState { minIndentation = 5</span></a>
<a class="sourceLine" id="cb15-10" title="10"><span class="co">--                                    , maxIndentation = 5</span></a>
<a class="sourceLine" id="cb15-11" title="11"><span class="co">--                                    , absMode = False</span></a>
<a class="sourceLine" id="cb15-12" title="12"><span class="co">--                                    , tokenRel = Ge}</span></a>
<a class="sourceLine" id="cb15-13" title="13"><span class="co">--                , tokenStream = &quot;&quot;}</span></a></code></pre></div>
<p>Now it works as expected</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb16-1" title="1">test4 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb16-2" title="2">  [ <span class="st">&quot;def foo(x, y):&quot;</span></a>
<a class="sourceLine" id="cb16-3" title="3">  , <span class="st">&quot;    add x&quot;</span></a>
<a class="sourceLine" id="cb16-4" title="4">  , <span class="st">&quot;     y&quot;</span></a>
<a class="sourceLine" id="cb16-5" title="5">  ]</a>
<a class="sourceLine" id="cb16-6" title="6"></a>
<a class="sourceLine" id="cb16-7" title="7">parse test4</a>
<a class="sourceLine" id="cb16-8" title="8"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a>
<a class="sourceLine" id="cb16-9" title="9">parse test1</a>
<a class="sourceLine" id="cb16-10" title="10"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>This library has other bits to it, but this should give enough to figure out, how to add indentation sensitivity to your language.</p>
<p>Special thanks to <a href="http://www.lambdageek.org/aleksey/">Aleksey Kliger</a> for helping me understand this library.</p>

</div>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Tue, 12 Jan 2016 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2016-01-12-indentation-sensitive-parsing/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>

    </channel>
</rss>
