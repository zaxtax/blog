<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Convex Optimized feed.</title>
        <link>http://zinkov.com</link>
        <description><![CDATA[Various ruminations on machine learning and mathematics]]></description>
        <atom:link href="http://zinkov.com/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Fri, 21 Dec 2018 00:00:00 UT</lastBuildDate>
        <item>
    <title>NeurIPS 2018: Papers to check out</title>
    <link>http://zinkov.com/posts/2018-12-21-nips-reviews/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>NeurIPS 2018: Papers to check out</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/" rel="me">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>NeurIPS 2018: Papers to check out</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2018-12-21</h5>
</div>

<div class="text-left">

<p>It’s been a long time since I’ve done one of these, so below are some of the papers I found exciting at this past NeurIPS. One notable thing is as the conference has gotten larger is there are simply more papers being presented. Some people worry that in becoming a gigantic conference the quality is declining but thanks to the great efforts of community, more people has meant more interesting ideas and more great papers to read. I typically highlight 5 papers</p>
<h4 id="approximate-inference"><a href="#approx">Approximate Inference</a></h4>
<h4 id="program-synthesis"><a href="#progsynth">Program Synthesis</a></h4>
<h4 id="applications"><a href="#app">Applications</a></h4>
<h4 id="misc"><a href="#misc">Misc</a></h4>
<hr />
<h2 id="conference-papers">Conference papers:</h2>
<a name="approx">
<h3>
Approximate Inference
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/7326-implicit-reparameterization-gradients"><strong>Implicit Reparameterization Gradients</strong></a></p>
<p><em>Mikhail Figurnov, Shakir Mohamed, Andriy Mnih</em></p>
<p><strong>Abstract:</strong> By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.</p>
<p><a href="http://papers.nips.cc/paper/7460-random-feature-stein-discrepancies"><strong>Random Feature Stein Discrepancies</strong></a></p>
<p><em>Jonathan Huggins, Lester Mackey</em></p>
<p><strong>Abstract:</strong> Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power—even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies (ΦSDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct ΦSDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations—random ΦSDs (RΦSDs)—which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, RΦSDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.</p>
<p><a href="http://papers.nips.cc/paper/7514-wasserstein-variational-inference"><strong>Wasserstein Variational Inference</strong></a></p>
<p><em>Luca Ambrogioni, Umut Güçlü, Yağmur Güçlütürk, Max Hinne, Marcel A. J. van Gerven, Eric Maris</em></p>
<p><strong>Abstract:</strong> This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.</p>
<p><a href="http://papers.nips.cc/paper/7632-deepproblog-neural-probabilistic-logic-programming"><strong>DeepProbLog: Neural Probabilistic Logic Programming</strong></a></p>
<p><em>Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt</em></p>
<p><strong>Abstract:</strong> We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.</p>
<p><a href="http://papers.nips.cc/paper/7669-meta-learning-mcmc-proposals"><strong>Meta-Learning MCMC Proposals</strong></a></p>
<p><em>Tongzhou Wang, Yi Wu, Dave Moore, Stuart J. Russell</em></p>
<p><strong>Abstract:</strong> Effective implementations of sampling-based probabilistic inference often require manually constructed, model-specific proposals. Inspired by recent progresses in meta-learning for training learning agents that can generalize to unseen environments, we propose a meta-learning approach to building effective and generalizable MCMC proposals. We parametrize the proposal as a neural network to provide fast approximations to block Gibbs conditionals. The learned neural proposals generalize to occurrences of common structural motifs across different models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler yields higher final F1 scores than classical single-site Gibbs sampling.</p>
<p><a href="http://papers.nips.cc/paper/7699-importance-weighting-and-variational-inference"><strong>Importance Weighting and Variational Inference</strong></a></p>
<p><em>Justin Domke, Daniel R. Sheldon</em></p>
<p><strong>Abstract:</strong> Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI’s practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.</p>
<p><a href="http://papers.nips.cc/paper/7935-gilbo-one-metric-to-measure-them-all"><strong>GILBO: One Metric to Measure Them All</strong></a></p>
<p><em>Alexander A. Alemi, Ian Fischer</em></p>
<p><strong>Abstract:</strong> We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and discuss the results.</p>
<p><a href="http://papers.nips.cc/paper/8041-graphical-model-inference-sequential-monte-carlo-meets-deterministic-approximations"><strong>Graphical model inference: Sequential Monte Carlo meets deterministic approximations</strong></a></p>
<p><em>Fredrik Lindsten, Jouni Helske, Matti Vihola</em></p>
<p><strong>Abstract:</strong> Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences, but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency, but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically, we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable, we show explicitly how this can be done with loopy belief propagation, expectation propagation, and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and, indeed, numerical results show clear improvements over the baseline deterministic methods as well as over “plain” SMC.</p>
<p><a href="http://papers.nips.cc/paper/8093-a-bayesian-nonparametric-view-on-count-min-sketch"><strong>A Bayesian Nonparametric View on Count-Min Sketch</strong></a></p>
<p><em>Diana Cai, Michael Mitzenmacher, Ryan P. Adams</em></p>
<p><strong>Abstract</strong> The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data with known ground truth, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.</p>
<p><a href="http://papers.nips.cc/paper/8270-autoconj-recognizing-and-exploiting-conjugacy-without-a-domain-specific-language"><strong>Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language</strong></a></p>
<p><em>Matthew D. Hoffman, Matthew J. Johnson, Dustin Tran</em></p>
<p><strong>Abstract</strong> Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies. The package can be downloaded at <a href="https://github.com/google-research/autoconj" class="uri">https://github.com/google-research/autoconj</a>.</p>
<p><a href="http://papers.nips.cc/paper/8015-robust-hypothesis-testing-using-wasserstein-uncertainty-sets"><strong>Robust Hypothesis Testing Using Wasserstein Uncertainty Sets</strong></a></p>
<p><em>Rui Gao, Liyan Xie, Yao Xie, Huan Xu</em></p>
<p><strong>Abstract</strong> We develop a novel computationally efficient and general framework for robust hypothesis testing. The new framework features a new way to construct uncertainty sets under the null and the alternative distributions, which are sets centered around the empirical distribution defined via Wasserstein metric, thus our approach is data-driven and free of distributional assumptions. We develop a convex safe approximation of the minimax formulation and show that such approximation renders a nearly-optimal detector among the family of all possible tests. By exploiting the structure of the least favorable distribution, we also develop a tractable reformulation of such approximation, with complexity independent of the dimension of observation space and can be nearly sample-size-independent in general. Real-data example using human activity data demonstrated the excellent performance of the new robust detector.</p>
<p><a href="http://papers.nips.cc/paper/7304-geometrically-coupled-monte-carlo-sampling"><strong>Geometrically Coupled Monte Carlo Sampling</strong></a></p>
<p><em>Mark Rowland, Krzysztof M. Choromanski, François Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E. Turner, Adrian Weller</em></p>
<p><strong>Abstract</strong> Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks. We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies. We compare our new strategies against prior methods for improving sample efficiency, including QMC, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.</p>
<p><a href="http://papers.nips.cc/paper/7769-assessing-generative-models-via-precision-and-recall"><strong>Assessing Generative Models via Precision and Recall</strong></a></p>
<p><em>Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly</em></p>
<p><strong>Abstract</strong> Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.</p>
<p><a href="http://papers.nips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning"><strong>DAGs with NO TEARS: Continuous Optimization for Structure Learning</strong></a></p>
<p><em>Xun Zheng, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing</em></p>
<p><strong>Abstract</strong> Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.</p>
<p><a href="http://papers.nips.cc/paper/7292-doubly-robust-bayesian-inference-for-non-stationary-streaming-data-with-beta-divergences"><strong>Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with β-Divergences</strong></a></p>
<p><em>Jeremias Knoblauch, Jack E. Jewson, Theodoros Damoulas</em></p>
<p><strong>Abstract</strong> We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with β-divergences. The resulting inference procedure is doubly robust for both the predictive and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as β→0. Secondly, we give a principled way of choosing the divergence parameter β by minimizing expected predictive loss on-line. Reducing False Discovery Rates of from up to 99% to 0% on real world data, this offers the state of the art.</p>
<p><a href="http://papers.nips.cc/paper/8048-the-promises-and-pitfalls-of-stochastic-gradient-langevin-dynamics"><strong>The promises and pitfalls of Stochastic Gradient Langevin Dynamics</strong></a></p>
<p><em>Nicolas Brosse, Alain Durmus, Eric Moulines</em></p>
<p><strong>Abstract</strong> Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated spectacular successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like as Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.</p>
<p><a href="http://papers.nips.cc/paper/7799-reparameterization-gradient-for-non-differentiable-models"><strong>Reparameterization Gradient for Non-differentiable Models</strong></a></p>
<p><em>Wonyeol Lee, Hangyeol Yu, Hongseok Yang</em></p>
<p><strong>Abstract</strong> We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary’s contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.</p>
<p><a href="http://papers.nips.cc/paper/8178-improving-explorability-in-variational-inference-with-annealed-variational-objectives"><strong>Improving Explorability in Variational Inference with Annealed Variational Objectives</strong></a></p>
<p><em>Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, Aaron C. Courville</em></p>
<p><strong>Abstract</strong> Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned. We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods. Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective. In our experiments, we demonstrate our method’s robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space.</p>
<a name="progsynth">
<h3>
Program Synthesis
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/7445-neural-guided-constraint-logic-programming-for-program-synthesis"><strong>Neural Guided Constraint Logic Programming for Program Synthesis</strong></a></p>
<p><em>Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William Byrd, Matthew Might, Raquel Urtasun, Richard Zemel</em></p>
<p><strong>Abstract</strong> Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren’s internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at <a href="https://github.com/xuexue/neuralkanren" class="uri">https://github.com/xuexue/neuralkanren</a>. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.</p>
<p><a href="http://papers.nips.cc/paper/7845-learning-to-infer-graphics-programs-from-hand-drawn-images"><strong>Learning to Infer Graphics Programs from Hand-Drawn Images</strong></a></p>
<p><em>Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, Josh Tenenbaum</em></p>
<p><strong>Abstract</strong> We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of .~The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a specification (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings.</p>
<p><a href="http://papers.nips.cc/paper/8006-learning-libraries-of-subroutines-for-neurallyguided-bayesian-program-induction"><strong>Learning Libraries of Subroutines for Neurally–Guided Bayesian Program Induction</strong></a></p>
<p><em>Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, Josh Tenenbaum</em></p>
<p><strong>Abstract</strong> Successful approaches to program induction require a hand-engineered domain-specific language (DSL), constraining the space of allowed programs and imparting prior knowledge of the domain. We contribute a program induction algorithm that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We use our model to synthesize functions on lists, edit text, and solve symbolic regression problems, showing how the model learns a domain-specific library of program components for expressing solutions to problems in the domain.</p>
<p><a href="http://papers.nips.cc/paper/8086-houdini-lifelong-learning-as-program-synthesis"><strong>HOUDINI: Lifelong Learning as Program Synthesis</strong></a></p>
<p><em>Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, Swarat Chaudhuri</em></p>
<p><strong>Abstract</strong> We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks signiﬁcantly accelerates the search.</p>
<p><a href="http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs"><strong>A Retrieve-and-Edit Framework for Predicting Structured Outputs</strong></a></p>
<p><em>Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy S. Liang</em></p>
<p><strong>Abstract</strong> For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.</p>
<p><a href="http://papers.nips.cc/paper/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics"><strong>Neural Code Comprehension: A Learnable Representation of Code Semantics</strong></a></p>
<p><em>Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler</em></p>
<p><strong>Abstract</strong> With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.</p>
<p><a href="http://papers.nips.cc/paper/7521-tree-to-tree-neural-networks-for-program-translation"><strong>Tree-to-tree Neural Networks for Program Translation</strong></a></p>
<p><em>Xinyun Chen, Chang Liu, Dawn Song</em></p>
<p><strong>Abstract</strong> Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.</p>
<p><a href="http://papers.nips.cc/paper/8107-improving-neural-program-synthesis-with-inferred-execution-traces"><strong>Improving Neural Program Synthesis with Inferred Execution Traces</strong></a></p>
<p><em>Richard Shin, Illia Polosukhin, Dawn Song</em></p>
<p><strong>Abstract</strong> The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, more so than other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces, which contain a superset of the information input/output pairs. While they are highly useful for program synthesis, as execution traces are more difficult to obtain than input/output pairs, we use the insight that we can split the process into two parts: infer the trace from the input/output example, then infer the program from the trace. This simple modification leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work.</p>
<p><a href="http://papers.nips.cc/paper/8204-memory-augmented-policy-optimization-for-program-synthesis-and-semantic-parsing"><strong>Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing</strong></a></p>
<p><em>Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V. Le, Ni Lao</em></p>
<p><strong>Abstract</strong> We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at <a href="https://goo.gl/TXBp4e" class="uri">https://goo.gl/TXBp4e</a></p>
<p><a href="http://papers.nips.cc/paper/7479-automatic-program-synthesis-of-long-programs-with-a-learned-garbage-collector"><strong>Automatic Program Synthesis of Long Programs with a Learned Garbage Collector</strong></a></p>
<p><em>Amit Zohar, Lior Wolf</em></p>
<p><strong>Abstract</strong> We consider the problem of generating automatic code given sample input-output pairs. We train a neural network to map from the current state and the outputs to the program’s next statement. The neural network optimizes multiple tasks concurrently: the next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude. Our code, including an implementation of various literature baselines, is publicly available at <a href="https://github.com/amitz25/PCCoder" class="uri">https://github.com/amitz25/PCCoder</a></p>
<p><a href="http://papers.nips.cc/paper/8001-learning-loop-invariants-for-program-verification"><strong>Learning Loop Invariants for Program Verification</strong></a></p>
<p><em>Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, Le Song</em></p>
<p><strong>Abstract</strong> A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework Code2Inv that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, Code2Inv captures rich program features and avoids the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, it addresses unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate Code2Inv on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 106 problems compared to 73 by a stochastic search-based system, 77 by a heuristic search-based system, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.</p>
<a name="app">
<h3>
Application papers
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/8189-scalable-end-to-end-autonomous-vehicle-testing-via-rare-event-simulation"><strong>Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation</strong></a></p>
<p><em>Matthew O’Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, John C. Duchi</em></p>
<p><strong>Abstract</strong> While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the de facto evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by 2-20 times over naive Monte Carlo sampling methods and 10-300P times (where P is the number of processors) over real-world testing.</p>
<p><a href="http://papers.nips.cc/paper/7877-graph-convolutional-policy-network-for-goal-directed-molecular-graph-generation"><strong>Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation</strong></a></p>
<p><em>Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, Jure Leskovec</em></p>
<p><strong>Abstract</strong> Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models that finds molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.</p>
<p><a href="http://papers.nips.cc/paper/8005-constrained-graph-variational-autoencoders-for-molecule-design"><strong>Constrained Graph Variational Autoencoders for Molecule Design</strong></a></p>
<p><em>Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, Alexander Gaunt</em></p>
<p><strong>Abstract</strong> Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.</p>
<a name="misc">
<h3>
Misc
</h3>
<p></a></p>
<p><a href="http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations"><strong>Neural Ordinary Differential Equations</strong></a></p>
<p><em>Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, David K. Duvenaud</em></p>
<p><strong>Abstract</strong> We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.</p>
<p><a href="http://papers.nips.cc/paper/7342-a-unified-framework-for-extensive-form-game-abstraction-with-bounds"><strong>A Unified Framework for Extensive-Form Game Abstraction with Bounds</strong></a></p>
<p><em>Christian Kroer, Tuomas Sandholm</em></p>
<p><strong>Abstract</strong> Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this, abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction, and specific solution concepts, for example, exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees—while maintaining comparable bounds on abstraction quality. Moreover, our framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as our results depend on the specific strategy chosen. Nonetheless, we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show, via counterexample, that such assumptions are necessary for some games. Finally, we prove the first bounds for how ϵ-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games.</p>
<p><a href="http://papers.nips.cc/paper/7698-exponentiated-strongly-rayleigh-distributions"><strong>Exponentiated Strongly Rayleigh Distributions</strong></a></p>
<p><em>Zelda E. Mariet, Suvrit Sra, Stefanie Jegelka</em></p>
<p><strong>Abstract</strong> Strongly Rayleigh (SR) measures are discrete probability distributions over the subsets of a ground set. They enjoy strong negative dependence properties, as a result of which they assign higher probability to subsets of diverse elements. We introduce in this paper Exponentiated Strongly Rayleigh (ESR) measures, which sharpen (or smoothen) the negative dependence property of SR measures via a single parameter (the exponent) that can intuitively understood as an inverse temperature. We develop efficient MCMC procedures for approximate sampling from ESRs, and obtain explicit mixing time bounds for two concrete instances: exponentiated versions of Determinantal Point Processes and Dual Volume Sampling. We illustrate some of the potential of ESRs, by applying them to a few machine learning tasks; empirical results confirm that beyond their theoretical appeal, ESR-based models hold significant promise for these tasks.</p>
<p><a href="http://papers.nips.cc/paper/7599-learning-to-optimize-tensor-programs"><strong>Learning to Optimize Tensor Programs</strong></a></p>
<p><em>Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy</em></p>
<p><strong>Abstract</strong> We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.</p>
<p><a href="http://papers.nips.cc/paper/7502-modelling-sparsity-heterogeneity-reciprocity-and-community-structure-in-temporal-interaction-data"><strong>Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data</strong></a></p>
<p><em>Xenia Miscouridou, Francois Caron, Yee Whye Teh</em></p>
<p><strong>Abstract</strong> We propose a novel class of network models for temporal dyadic interaction data. Our objective is to capture important features often observed in social interactions: sparsity, degree heterogeneity, community structure and reciprocity. We use mutually-exciting Hawkes processes to model the interactions between each (directed) pair of individuals. The intensity of each process allows interactions to arise as responses to opposite interactions (reciprocity), or due to shared interests between individuals (community structure). For sparsity and degree heterogeneity, we build the non time dependent part of the intensity function on compound random measures following Todeschini et al., 2016. We conduct experiments on real-world temporal interaction data and show that the proposed model outperforms competing approaches for link prediction, and leads to interpretable parameters.</p>
<h2 id="workshop-papers">Workshop papers:</h2>
<p>Infer to Control workshop</p>
<p><a href="https://arxiv.org/abs/1811.01132"><strong>VIREL: A Variational Inference Framework for Reinforcement Learning</strong></a></p>
<p><em>Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner, Shimon Whiteson</em></p>
<p><strong>Abstract</strong> Applying probabilistic models to reinforcement learning (RL) has become an exciting direction of research owing to powerful optimisation tools such as variational inference becoming applicable to RL. However, due to their formulation, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, for example, the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties in optimisation of learning objective in maximum entropy RL based approaches. We propose VIREL, a novel, theoretically grounded probabilistic inference framework for RL that utilises the action-value function in a parametrised form to capture future dynamics of the underlying Markov decision process. Owing to its generality, our framework lends itself to current advances in variational inference. Applying the variational expectation-maximisation algorithm to our framework, we show that the actor-critic algorithm can be reduced to expectation-maximisation. We derive a family of methods from our framework, including state-of-the-art methods based on soft value functions. We evaluate two actor-critic algorithms derived from this family, which perform on par with soft actor critic, demonstrating that our framework offers a promising perspective on RL as inference.</p>

</div>

<div id="disqus_thread"></div>
<script>
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://convexoptimized.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Fri, 21 Dec 2018 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2018-12-21-nips-reviews/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Decomposing the ELBO</title>
    <link>http://zinkov.com/posts/2018-11-02-decomposing-the-elbo/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Decomposing the ELBO</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/" rel="me">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Decomposing the ELBO</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2018-11-02</h5>
</div>

<div class="text-left">

<p>When performing Variational Inference, we are minimizing the KL divergence between some distribution we care about <span class="math inline">\(p(\v{z} \mid \v{x})\)</span> and some distribution that is easier to work with <span class="math inline">\(q_\phi(\v{z} \mid \v{x})\)</span>.</p>
<p><span class="math display">\[
\begin{align}
	\phi^* &amp;= \underset{\phi}{\mathrm{argmin}}\, \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z} \mid \v{x})) \\
		&amp;= \underset{\phi}{\mathrm{argmin}}\,
  \mathbb{E}_{q_\phi(\v{z} \mid \v{x})}
  \big[\log q_\phi(\v{z} \mid \v{x})
  -
  \log p(\v{z} \mid \v{x})
  \big]\\
\end{align}
\]</span></p>
<p>Now because the density of <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span> usually isn’t tractable, we use a property of the log model evidence <span class="math inline">\(\log\, p(\v{x})\)</span> to define a different objective to optimize.</p>
<p><span class="math display">\[
\begin{align}
\Expect_{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{z} \mid \v{x})\big]
&amp;\leq \Expect_{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{z} \mid \v{x})\big] - \log p(\v{x}) \\
&amp;= \Expect _{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{z} \mid \v{x}) - \log p(\v{x})\big] \\
&amp;= \Expect _{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x}) - \log p(\v{x}, \v{z})\big]\\
&amp;= -\mathcal{L}(\phi)
\end{align}
\]</span></p>
<p>As <span class="math inline">\(\mathcal{L}(\phi) = \log p(\v{x}) - \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z} \mid \v{x}))\)</span> maximizing <span class="math inline">\(\mathcal{L}(\phi)\)</span> effectively minimizes our original KL.</p>
<p>This term <span class="math inline">\(\mathcal{L}(\phi)\)</span> is sometimes called the evidence lower-bound or ELBO, because the KL term must always be greater-than or equal to zero, <span class="math inline">\(\mathcal{L}(\phi)\)</span> can be seen as a lower-bound estimate of <span class="math inline">\(\log p(\v{x})\)</span>.</p>
<p>Due to various linearity properties of expectations, this can be rearranged into many different forms. This is useful to get an intuition for what can be going wrong when you learn <span class="math inline">\(q_\phi(\v{z} \mid \v{x})\)</span></p>
<p>Now why does this matter? Couldn’t I just optimize this loss with SGD and be done? Well you can, but often if something is going wrong it will show up as one or some terms being unusually off. By making these tradeoffs in the loss function explicit means you can adjust it to favor different properties of your learned representation. Either by hand or automatically.</p>
<h2 id="entropy-form">Entropy form</h2>
<p>The classic form is in terms of an energy term and an entropy term. The first term encourages <span class="math inline">\(q\)</span> to put high probability mass wherever <span class="math inline">\(p\)</span> does so. The second term is encouraging that <span class="math inline">\(q\)</span> should as much as possible maximize it’s entropy and put probability mass everywhere it can.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}[\log p(x, z)] + H(q_\phi(\v{z} \mid \v{x})) \]</span></p>
<p>where</p>
<p><span class="math display">\[ H(q_\phi(\v{z} \mid \v{x})) \triangleq - \Expect_{q_\phi(\v{z} \mid \v{x})}[\log q_\phi(\v{z} \mid \v{x})] \]</span></p>
<h2 id="reconstruction-error-minus-kl-on-the-prior">Reconstruction error minus KL on the prior</h2>
<p>More often these days, we describe the <span class="math inline">\(\mathcal{L}\)</span> in terms of a reconstruction term and KL on the prior for <span class="math inline">\(p\)</span>. Here the first term is saying we should put mass on latent codes <span class="math inline">\(\v{z}\)</span> from which <span class="math inline">\(p\)</span> is likely to generate our observation <span class="math inline">\(\v{x}\)</span>. The second term then suggests to this trade off with <span class="math inline">\(q\)</span> also being near the prior.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}[\log p(\v{x} \mid \v{z})] - \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z}))\]</span></p>
<h2 id="elbo-surgery">ELBO surgery</h2>
<p>But there are other ways to think about this decomposition. Because we frequently use amortized inference to learn a <span class="math inline">\(\phi\)</span> useful for describing all kinds of <span class="math inline">\(q\)</span> distributions regardless of our choice of observation <span class="math inline">\(\v{x}\)</span>. We can talk about the average distribution we learn over our observed data, with <span class="math inline">\(p_d\)</span> being the empirical distribution of our observations.</p>
<p><span class="math display">\[ \overline{q}_\phi(\v{z}) = \Expect_{p_d(\v{x})} \big[ q_\phi(\v{z} \mid \v{x}) \big] \]</span></p>
<p>This is sometimes called the aggregate posterior.</p>
<p>With that we can decompose our KL on the prior into a mutual information term that encourages each <span class="math inline">\(q_\phi(\v{z} \mid \v{x})\)</span> we create to be near the average one <span class="math inline">\(\overline{q}_\phi(\v{z})\)</span> and a KL between this average distribution and the prior. The encourages the representation generated for <span class="math inline">\(\v{z}\)</span> to be useful.</p>
<p><span class="math display">\[ w\mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}[\log p(\v{x} \mid \v{z})] - \mathbb{I}_q(\v{x},\v{z})  - \text{KL}(\overline{q}_\phi(\v{z}) \;\|\; p(\v{z})) \]</span></p>
<p>where</p>
<p><span class="math display">\[ \mathbb{I}_q(\v{x},\v{z}) \triangleq \Expect_{p_d}\big[\Expect_{q_\phi(\v{z} \mid \v{x})} \big[\log q_\phi(\v{z} \mid \v{x})\big] \big]
- \Expect_{\overline{q}_\phi(\v{z})} \log \overline{q}_\phi(\v{z}) \]</span></p>
<h2 id="difference-of-two-kl-divergences">Difference of two KL divergences</h2>
<p>With something like <span class="math inline">\(p_d\)</span> around it is also possible to pull out the relationship between <span class="math inline">\(p\)</span> and <span class="math inline">\(p_d\)</span>. This is particularly relevant if you intend to learn <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = - \text{KL}(q_\phi(\v{z} \mid \v{x}) \;\|\; p(\v{z} \mid \v{x})) - \text{KL}(p_d(\v{x}) \;\|\; p(\v{x})) \]</span></p>
<h2 id="full-decomposition">Full decomposition</h2>
<p>Of course with more aggressive rearranging, we can just have a term to encourage learning better latent representations. In a setting where you aren’t learning <span class="math inline">\(p\)</span> some of these terms are constant and can generally be ignored. I provide them here for completeness.</p>
<p><span class="math display">\[ \mathcal{L}(\phi) = \Expect_{q_\phi(\v{z} \mid \v{x})}\left[ \log\frac{p(\v{x} \mid \v{z})}{p(\v{x})}
- \log\frac{q_\phi(\v{z} \mid \v{x})}{q_\phi(\v{z})} \right]
- \text{KL}(p_d(\v{x}) \;\|\; p(\v{x}))
- \text{KL}(\overline{q}_\phi(\v{z}) \;\|\; p(\v{z}))\]</span></p>
<p>I highly encourage checking out the Appendix of the Structured Disentangled Representations paper to see how much further this can be pushed.</p>
<h2 id="final-notes">Final notes</h2>
<p>Of course, all the above still holds in the VAE setting where <span class="math inline">\(p\)</span> becomes <span class="math inline">\(p_\theta\)</span> but I felt the notation was cluttered enough already. It’s kind of amazing how much insight can be gained through expanding and collapsing one loss function.</p>
<h2 id="further-references">Further references</h2>
<ul>
<li><a href="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf">ELBO Surgery: yet another way to carve up the variational evidence lower bound</a></li>
<li><a href="https://arxiv.org/pdf/1711.00464.pdf">Fixing a Broken ELBO</a></li>
<li><a href="https://arxiv.org/pdf/1804.02086.pdf">Structured Disentangled Representations</a></li>
<li><a href="https://arxiv.org/abs/1511.06349">Generating Sentences from a Continuous Space</a></li>
<li><a href="https://arxiv.org/abs/1702.08658">Towards Deeper Understanding of Variational Autoencoding Models</a></li>
</ul>

</div>

<div id="disqus_thread"></div>
<script>
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://convexoptimized.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Fri, 02 Nov 2018 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2018-11-02-decomposing-the-elbo/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Calculating the golden-era of The Simpsons</title>
    <link>http://zinkov.com/posts/2017-11-03-simpsons-changepoint/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Calculating the golden-era of The Simpsons</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/" rel="me">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Calculating the golden-era of The Simpsons</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2017-11-03</h5>
</div>

<div class="text-left">

<p><a href="http://www.nathancunn.com/">Nathan Cunningham</a> published last week a <a href="http://www.nathancunn.com/2017-10-26-simpsons-decline/">fantastic</a> article about using some stats to estimate at what episode did Simpsons start to decline.</p>
<p>Cameron Davidson-Pilon suggested this would make a great application of Bayesian changepoint models.</p>
<blockquote class="twitter-tweet" data-lang="en-gb">
<p lang="en" dir="ltr">
Someone want to Bayesian switchpoint model this? See first chapter of BMH <a href="https://t.co/QiGwzA0khD">https://t.co/QiGwzA0khD</a>
</p>
— Cam DP 👨🏽‍💻 (<span class="citation" data-cites="Cmrn_DP">@Cmrn_DP</span>) <a href="https://twitter.com/Cmrn_DP/status/924360674041585664?ref_src=twsrc%5Etfw">28 October 2017</a>
</blockquote>
<p>In turns out, he was totally right. Taking the <a href="http://docs.pymc.io/notebooks/getting_started.html#Case-study-2:-Coal-mining-disasters">Coal-mining disaster</a> example from the pymc3 quickstart guide and slightly modifying it is enough to do the job.</p>
<p>First we load the data</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">data <span class="op">=</span> pd.read_csv(<span class="st">&quot;simpsons_ratings.csv&quot;</span>)</a>
<a class="sourceLine" id="cb1-2" title="2">index <span class="op">=</span> data.index</a></code></pre></div>
<p>Then we use some Gaussians to describe the average rating, and how that mean rate translates to the quality of any particular episode.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</a>
<a class="sourceLine" id="cb2-2" title="2">    switch <span class="op">=</span> pm.DiscreteUniform(<span class="st">'switch'</span>, lower<span class="op">=</span>index.<span class="bu">min</span>(), upper<span class="op">=</span>index.<span class="bu">max</span>())</a>
<a class="sourceLine" id="cb2-3" title="3">    early_mean <span class="op">=</span> pm.Normal(<span class="st">'early_mean'</span>, mu<span class="op">=</span><span class="fl">5.</span>, sd<span class="op">=</span><span class="fl">1.</span>)</a>
<a class="sourceLine" id="cb2-4" title="4">    late_mean <span class="op">=</span> pm.Normal(<span class="st">'late_mean'</span>, mu<span class="op">=</span><span class="fl">5.</span>, sd<span class="op">=</span><span class="fl">1.</span>)</a>
<a class="sourceLine" id="cb2-5" title="5">    mean <span class="op">=</span> tt.switch(switch <span class="op">&gt;=</span> index.values, early_mean, late_mean)</a>
<a class="sourceLine" id="cb2-6" title="6">    ratings <span class="op">=</span> pm.Normal(<span class="st">'ratings'</span>, mu<span class="op">=</span>mean, sd<span class="op">=</span><span class="fl">1.</span>,</a>
<a class="sourceLine" id="cb2-7" title="7">                        observed<span class="op">=</span>data[<span class="st">&quot;UserRating&quot;</span>].values)</a>
<a class="sourceLine" id="cb2-8" title="8"></a>
<a class="sourceLine" id="cb2-9" title="9">    tr <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">500</span>)</a>
<a class="sourceLine" id="cb2-10" title="10">    pm.traceplot(tr)</a></code></pre></div>
<p><img src="../../images/simpsons_trace.png" /></p>
<p>As we can see around 220 is when our model thinks the Simpsons was starting to downward slide.</p>
<p>That would be</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(data[<span class="st">&quot;EpisodeID&quot;</span>][<span class="dv">220</span>], data[<span class="st">&quot;Episode&quot;</span>][<span class="dv">220</span>]))</a>
<a class="sourceLine" id="cb3-2" title="2"><span class="co"># &gt;&gt;&gt; S10E18: Simpsons Bible Stories</span></a></code></pre></div>
<p>An episode I remember being <em>alright</em>. Generally the 10th season is acknowledged as the last of the golden years. In fact, <a href="https://twitter.com/woohootriviachi">Chicago Simpsons Trivia Night</a> bills itself as not asking any questions from seasons after 10.</p>
<p>Apologies in advance for not using more Simpsons jokes in this post. You can find the code and data I used on <a href="https://github.com/zaxtax/simpsons_changepoint">github</a>.</p>

</div>

<div id="disqus_thread"></div>
<script>
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://convexoptimized.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Fri, 03 Nov 2017 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2017-11-03-simpsons-changepoint/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Indentation sensitive parsing the easy way</title>
    <link>http://zinkov.com/posts/2016-01-12-indentation-sensitive-parsing/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Indentation sensitive parsing the easy way</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/" rel="me">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Indentation sensitive parsing the easy way</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2016-01-12</h5>
</div>

<div class="text-left">

<p>Recently, I had to write an parser for <a href="https://github.com/hakaru-dev/hakaru">Hakaru</a>. Writing parsers in Haskell is generally a treat as there are muliple parser libraries to choose from including Happy, parsec, attoparsec, megaparsec, trifecta, and many others. The trouble occurs when you want to parse and indentation-sensitive language like Python or Haskell. For that task the choices are more limited and far less documented. Which is unfortunate as my favorite library <a href="https://hackage.haskell.org/package/indentation">indentation</a> of the bunch is the least documented. The following is how to use <code>indentation</code> to write an indentation-sensitive parser.</p>
<p>For this tutorial, I will use <code>indentation</code> and <code>parsec</code>.</p>
<pre><code>cabal install indentation parsec</code></pre>
<p>To get started import Parsec as you normally would</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" title="1"><span class="kw">module</span> <span class="dt">Indent.Demo</span> <span class="kw">where</span></a>
<a class="sourceLine" id="cb2-2" title="2"></a>
<a class="sourceLine" id="cb2-3" title="3"><span class="kw">import</span>           <span class="dt">Data.Functor</span>                  ((&lt;$&gt;), (&lt;$))</a>
<a class="sourceLine" id="cb2-4" title="4"><span class="kw">import</span>           <span class="dt">Control.Applicative</span>           (<span class="dt">Applicative</span>(..))</a>
<a class="sourceLine" id="cb2-5" title="5"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Control.Monad</span>                 <span class="kw">as</span> <span class="dt">M</span></a>
<a class="sourceLine" id="cb2-6" title="6"><span class="kw">import</span>           <span class="dt">Data.Functor.Identity</span></a>
<a class="sourceLine" id="cb2-7" title="7"><span class="kw">import</span>           <span class="dt">Data.Text</span>                     (<span class="dt">Text</span>)</a>
<a class="sourceLine" id="cb2-8" title="8"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Data.Text</span>                     <span class="kw">as</span> <span class="dt">Text</span></a>
<a class="sourceLine" id="cb2-9" title="9"><span class="kw">import</span>           <span class="dt">Text.Parsec</span>                   <span class="kw">hiding</span> (<span class="dt">Empty</span>)</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="kw">import</span>           <span class="dt">Text.Parsec.Text</span>              () <span class="co">-- instances only</span></a>
<a class="sourceLine" id="cb2-11" title="11"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Text.Parsec.Expr</span>              <span class="kw">as</span> <span class="dt">Ex</span></a>
<a class="sourceLine" id="cb2-12" title="12"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Text.Parsec.Token</span>             <span class="kw">as</span> <span class="dt">Tok</span></a></code></pre></div>
<p>And then add the following modules from <code>indentation</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">import</span>           <span class="dt">Text.Parsec.Indentation</span></a>
<a class="sourceLine" id="cb3-2" title="2"><span class="kw">import</span>           <span class="dt">Text.Parsec.Indentation.Char</span></a>
<a class="sourceLine" id="cb3-3" title="3"><span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Text.Parsec.Indentation.Token</span> <span class="kw">as</span> <span class="dt">ITok</span></a></code></pre></div>
<p>The key thing which needs to be changed is that the lexer needs to be indentation-sensitive. Sadly, there is no easy way to extend the existing LanguageDefs, so we make one from scratch.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" title="1"><span class="ot">style ::</span> <span class="dt">Tok.GenLanguageDef</span> <span class="dt">ParserStream</span> st <span class="dt">Identity</span></a>
<a class="sourceLine" id="cb4-2" title="2">style <span class="ot">=</span> ITok.makeIndentLanguageDef <span class="op">$</span> <span class="dt">Tok.LanguageDef</span></a>
<a class="sourceLine" id="cb4-3" title="3">    { Tok.commentStart    <span class="ot">=</span> <span class="st">&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-4" title="4">    , Tok.commentEnd      <span class="ot">=</span> <span class="st">&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-5" title="5">    , Tok.nestedComments  <span class="ot">=</span> <span class="dt">True</span></a>
<a class="sourceLine" id="cb4-6" title="6">    , Tok.identStart      <span class="ot">=</span> letter <span class="op">&lt;|&gt;</span> char <span class="ch">'_'</span></a>
<a class="sourceLine" id="cb4-7" title="7">    , Tok.identLetter     <span class="ot">=</span> alphaNum <span class="op">&lt;|&gt;</span> oneOf <span class="st">&quot;_'&quot;</span></a>
<a class="sourceLine" id="cb4-8" title="8">    , Tok.opStart         <span class="ot">=</span> oneOf <span class="st">&quot;!#$%&amp;*+./&lt;=&gt;?@\\^|-~&quot;</span></a>
<a class="sourceLine" id="cb4-9" title="9">    , Tok.opLetter        <span class="ot">=</span> oneOf <span class="st">&quot;!#$%&amp;*+./&lt;=&gt;?@\\^|-~&quot;</span></a>
<a class="sourceLine" id="cb4-10" title="10">    , Tok.caseSensitive   <span class="ot">=</span> <span class="dt">True</span></a>
<a class="sourceLine" id="cb4-11" title="11">    , Tok.commentLine     <span class="ot">=</span> <span class="st">&quot;#&quot;</span></a>
<a class="sourceLine" id="cb4-12" title="12">    , Tok.reservedOpNames <span class="ot">=</span> [<span class="st">&quot;:&quot;</span>]</a>
<a class="sourceLine" id="cb4-13" title="13">    , Tok.reservedNames   <span class="ot">=</span> [<span class="st">&quot;def&quot;</span>, <span class="st">&quot;add&quot;</span>]</a>
<a class="sourceLine" id="cb4-14" title="14">    }</a>
<a class="sourceLine" id="cb4-15" title="15"></a>
<a class="sourceLine" id="cb4-16" title="16"><span class="ot">lexer ::</span> <span class="dt">Tok.GenTokenParser</span> <span class="dt">ParserStream</span> () <span class="dt">Identity</span></a>
<a class="sourceLine" id="cb4-17" title="17">lexer <span class="ot">=</span> ITok.makeTokenParser style</a></code></pre></div>
<p>Once you have an indentation-sensitive lexer, you can add the primitives you need in terms of it.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" title="1"><span class="ot">integer ::</span> <span class="dt">Parser</span> <span class="dt">Integer</span></a>
<a class="sourceLine" id="cb5-2" title="2">integer <span class="ot">=</span> Tok.integer lexer</a>
<a class="sourceLine" id="cb5-3" title="3"></a>
<a class="sourceLine" id="cb5-4" title="4"><span class="ot">identifier ::</span> <span class="dt">Parser</span> <span class="dt">String</span></a>
<a class="sourceLine" id="cb5-5" title="5">identifier <span class="ot">=</span> Tok.identifier lexer</a>
<a class="sourceLine" id="cb5-6" title="6"></a>
<a class="sourceLine" id="cb5-7" title="7"><span class="ot">reserved ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Parser</span> ()</a>
<a class="sourceLine" id="cb5-8" title="8">reserved <span class="ot">=</span> Tok.reserved lexer</a>
<a class="sourceLine" id="cb5-9" title="9"></a>
<a class="sourceLine" id="cb5-10" title="10"><span class="ot">reservedOp ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Parser</span> ()</a>
<a class="sourceLine" id="cb5-11" title="11">reservedOp <span class="ot">=</span> Tok.reservedOp lexer</a>
<a class="sourceLine" id="cb5-12" title="12"></a>
<a class="sourceLine" id="cb5-13" title="13"><span class="ot">parens ::</span> <span class="dt">Parser</span> a <span class="ot">-&gt;</span> <span class="dt">Parser</span> a</a>
<a class="sourceLine" id="cb5-14" title="14">parens <span class="ot">=</span> Tok.parens lexer <span class="op">.</span> localIndentation <span class="dt">Any</span></a>
<a class="sourceLine" id="cb5-15" title="15"></a>
<a class="sourceLine" id="cb5-16" title="16"><span class="ot">commaSep ::</span> <span class="dt">Parser</span> a <span class="ot">-&gt;</span> <span class="dt">Parser</span> [a]</a>
<a class="sourceLine" id="cb5-17" title="17">commaSep <span class="ot">=</span> Tok.commaSep lexer</a></code></pre></div>
<p>All of these are boilerplate except for <code>parens</code>. You will notice, for it we call <code>localIndentation Any</code> before passing the input. This function indicates that indentation rules can be ignored when using this combinator. This gives parentheses the meaning they have in python which is to suspend indentation rules. We will go into more detail how the indentation primitives work, but for now let’s define AST for our language.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">type</span> <span class="dt">Name</span> <span class="ot">=</span> <span class="dt">String</span></a>
<a class="sourceLine" id="cb6-2" title="2"><span class="kw">type</span> <span class="dt">Args</span> <span class="ot">=</span> [<span class="dt">Name</span>]</a>
<a class="sourceLine" id="cb6-3" title="3"></a>
<a class="sourceLine" id="cb6-4" title="4"><span class="kw">type</span> <span class="dt">ParserStream</span>    <span class="ot">=</span> <span class="dt">IndentStream</span> (<span class="dt">CharIndentStream</span> <span class="dt">String</span>)</a>
<a class="sourceLine" id="cb6-5" title="5"><span class="kw">type</span> <span class="dt">Parser</span>          <span class="ot">=</span> <span class="dt">ParsecT</span>     <span class="dt">ParserStream</span> () <span class="dt">Identity</span></a>
<a class="sourceLine" id="cb6-6" title="6"></a>
<a class="sourceLine" id="cb6-7" title="7"><span class="kw">data</span> <span class="dt">Expr</span> <span class="ot">=</span></a>
<a class="sourceLine" id="cb6-8" title="8">     <span class="dt">Func</span> <span class="dt">Name</span> <span class="dt">Args</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb6-9" title="9">   <span class="op">|</span> <span class="dt">Var</span>  <span class="dt">Name</span></a>
<a class="sourceLine" id="cb6-10" title="10">   <span class="op">|</span> <span class="dt">App</span>  <span class="dt">Expr</span> [<span class="dt">Expr</span>]</a>
<a class="sourceLine" id="cb6-11" title="11">   <span class="op">|</span> <span class="dt">Add</span>  <span class="dt">Expr</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb6-12" title="12">   <span class="op">|</span> <span class="dt">Lit</span>  <span class="dt">Integer</span></a>
<a class="sourceLine" id="cb6-13" title="13">   <span class="kw">deriving</span> (<span class="dt">Show</span>)</a></code></pre></div>
<p>Parsing this language doesn’t involve need to involve indentation rules</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" title="1"><span class="ot">int ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-2" title="2">int <span class="ot">=</span> <span class="dt">Lit</span> <span class="op">&lt;$&gt;</span> integer</a>
<a class="sourceLine" id="cb7-3" title="3"></a>
<a class="sourceLine" id="cb7-4" title="4"><span class="ot">add ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-5" title="5">add <span class="ot">=</span> reserved <span class="st">&quot;add&quot;</span> <span class="op">*&gt;</span> (<span class="dt">Add</span> <span class="op">&lt;$&gt;</span> expr <span class="op">&lt;*&gt;</span> expr)</a>
<a class="sourceLine" id="cb7-6" title="6"></a>
<a class="sourceLine" id="cb7-7" title="7"><span class="ot">var ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-8" title="8">var <span class="ot">=</span> <span class="dt">Var</span> <span class="op">&lt;$&gt;</span> identifier</a>
<a class="sourceLine" id="cb7-9" title="9"></a>
<a class="sourceLine" id="cb7-10" title="10"><span class="ot">app ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-11" title="11">app <span class="ot">=</span> <span class="dt">App</span> <span class="op">&lt;$&gt;</span> var <span class="op">&lt;*&gt;</span> parens (commaSep expr)</a>
<a class="sourceLine" id="cb7-12" title="12"></a>
<a class="sourceLine" id="cb7-13" title="13"><span class="ot">def ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-14" title="14">def <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb7-15" title="15">  reserved <span class="st">&quot;def&quot;</span></a>
<a class="sourceLine" id="cb7-16" title="16">  name <span class="ot">&lt;-</span> identifier</a>
<a class="sourceLine" id="cb7-17" title="17">  args <span class="ot">&lt;-</span> parens (commaSep identifier)</a>
<a class="sourceLine" id="cb7-18" title="18">  body <span class="ot">&lt;-</span> reservedOp <span class="st">&quot;:&quot;</span> <span class="op">*&gt;</span> expr</a>
<a class="sourceLine" id="cb7-19" title="19">  <span class="fu">return</span> (<span class="dt">Func</span> name args body)</a>
<a class="sourceLine" id="cb7-20" title="20"></a>
<a class="sourceLine" id="cb7-21" title="21"><span class="ot">expr ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb7-22" title="22">expr <span class="ot">=</span> def</a>
<a class="sourceLine" id="cb7-23" title="23">   <span class="op">&lt;|&gt;</span> try app</a>
<a class="sourceLine" id="cb7-24" title="24">   <span class="op">&lt;|&gt;</span> try var</a>
<a class="sourceLine" id="cb7-25" title="25">   <span class="op">&lt;|&gt;</span> try add</a>
<a class="sourceLine" id="cb7-26" title="26">   <span class="op">&lt;|&gt;</span> int</a>
<a class="sourceLine" id="cb7-27" title="27">   <span class="op">&lt;|&gt;</span> parens expr</a></code></pre></div>
<p>Let’s add some helper code.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb8-1" title="1"><span class="ot">indentConfig ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">ParserStream</span></a>
<a class="sourceLine" id="cb8-2" title="2">indentConfig <span class="ot">=</span></a>
<a class="sourceLine" id="cb8-3" title="3">    mkIndentStream <span class="dv">0</span> infIndentation <span class="dt">True</span> <span class="dt">Ge</span> <span class="op">.</span> mkCharIndentStream</a>
<a class="sourceLine" id="cb8-4" title="4"></a>
<a class="sourceLine" id="cb8-5" title="5"><span class="ot">parse ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Either</span> <span class="dt">ParseError</span> [<span class="dt">Expr</span>]</a>
<a class="sourceLine" id="cb8-6" title="6">parse <span class="ot">=</span></a>
<a class="sourceLine" id="cb8-7" title="7">    runParser (many expr <span class="op">&lt;*</span> eof) () <span class="st">&quot;[input]&quot;</span> <span class="op">.</span> indentConfig</a></code></pre></div>
<p>And this parses programs just fine.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb9-1" title="1">test1 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb9-2" title="2">  [ <span class="st">&quot;def foo(x,y):&quot;</span></a>
<a class="sourceLine" id="cb9-3" title="3">  , <span class="st">&quot;    add x y&quot;</span></a>
<a class="sourceLine" id="cb9-4" title="4">  ]</a>
<a class="sourceLine" id="cb9-5" title="5"></a>
<a class="sourceLine" id="cb9-6" title="6">parse test1</a>
<a class="sourceLine" id="cb9-7" title="7"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>The issue is also things which feel invalid.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb10-1" title="1">test2 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb10-2" title="2">  [ <span class="st">&quot;def foo(x,y):&quot;</span></a>
<a class="sourceLine" id="cb10-3" title="3">  , <span class="st">&quot;add x y&quot;</span></a>
<a class="sourceLine" id="cb10-4" title="4">  ]</a>
<a class="sourceLine" id="cb10-5" title="5"></a>
<a class="sourceLine" id="cb10-6" title="6">parse test2</a>
<a class="sourceLine" id="cb10-7" title="7"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>We need to change <code>def</code> so that its body must be indented at strictly greater than character where it starts.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb11-1" title="1"><span class="ot">blockExpr ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb11-2" title="2">blockExpr <span class="ot">=</span> reservedOp <span class="st">&quot;:&quot;</span> <span class="op">*&gt;</span> localIndentation <span class="dt">Gt</span> expr</a>
<a class="sourceLine" id="cb11-3" title="3"></a>
<a class="sourceLine" id="cb11-4" title="4"><span class="ot">def ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb11-5" title="5">def <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb11-6" title="6">  reserved <span class="st">&quot;def&quot;</span></a>
<a class="sourceLine" id="cb11-7" title="7">  name <span class="ot">&lt;-</span> identifier</a>
<a class="sourceLine" id="cb11-8" title="8">  args <span class="ot">&lt;-</span> parens (commaSep identifier)</a>
<a class="sourceLine" id="cb11-9" title="9">  body <span class="ot">&lt;-</span> blockExpr</a>
<a class="sourceLine" id="cb11-10" title="10">  <span class="fu">return</span> (<span class="dt">Func</span> name args body)</a></code></pre></div>
<p>If you now look, we have defined a function for the body, <code>blockExpr</code>, which says we must have the body strictly greater. Now when we parse <code>test2</code> we get the following.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb12-1" title="1">parse test2</a>
<a class="sourceLine" id="cb12-2" title="2"><span class="co">-- Left &quot;[input]&quot; (line 2, column 2):</span></a>
<a class="sourceLine" id="cb12-3" title="3"><span class="co">-- expecting identifier</span></a>
<a class="sourceLine" id="cb12-4" title="4"><span class="co">--</span></a>
<a class="sourceLine" id="cb12-5" title="5"><span class="co">-- Invalid indentation.</span></a>
<a class="sourceLine" id="cb12-6" title="6"><span class="co">--   Found a token at indentation 1.</span></a>
<a class="sourceLine" id="cb12-7" title="7"><span class="co">--   Expecting a token at an indentation greater than or equal to 2.</span></a>
<a class="sourceLine" id="cb12-8" title="8"><span class="co">--   IndentStream { indentationState =</span></a>
<a class="sourceLine" id="cb12-9" title="9"><span class="co">--                   IndentationState { minIndentation = 2</span></a>
<a class="sourceLine" id="cb12-10" title="10"><span class="co">--                                    , maxIndentation = 9223372036854775807</span></a>
<a class="sourceLine" id="cb12-11" title="11"><span class="co">--                                    , absMode = False</span></a>
<a class="sourceLine" id="cb12-12" title="12"><span class="co">--                                    , tokenRel = Ge}</span></a>
<a class="sourceLine" id="cb12-13" title="13"><span class="co">--                , tokenStream = &quot;&quot;}</span></a></code></pre></div>
<p><code>localIndentation</code> takes two arguments, what the indentation of an expression should be relative to the current indentation, and the expression itself. Relative indentations can be greater-than and equal (Ge), strictly greater-than (Gt), equal (Eq), a specific amount (Const 5), or anything (Any).</p>
<p>While it seems like this is the only primitive you should need, sometimes the indentation level you want can’t be defined in terms of the parent.</p>
<p>For example, the following is a valid program</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb13-1" title="1">test3 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb13-2" title="2">  [ <span class="st">&quot;def foo(x, y):&quot;</span></a>
<a class="sourceLine" id="cb13-3" title="3">  , <span class="st">&quot;    add x&quot;</span></a>
<a class="sourceLine" id="cb13-4" title="4">  , <span class="st">&quot; y&quot;</span></a>
<a class="sourceLine" id="cb13-5" title="5">  ]</a>
<a class="sourceLine" id="cb13-6" title="6"></a>
<a class="sourceLine" id="cb13-7" title="7">parse test4</a>
<a class="sourceLine" id="cb13-8" title="8"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>The issue is that “y” is indented greater than the “def” but, we really want it to be indented in terms of “add”. To do this we need to use absolute indentation. This mode says indentation is defined in terms of the first token parsed, and all indentation rules apply in terms of where that first token is found.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb14-1" title="1"><span class="ot">absBlockExpr ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb14-2" title="2">absBlockExpr <span class="ot">=</span> reservedOp <span class="st">&quot;:&quot;</span> <span class="op">*&gt;</span> localIndentation <span class="dt">Gt</span> (absoluteIndentation expr)</a>
<a class="sourceLine" id="cb14-3" title="3"></a>
<a class="sourceLine" id="cb14-4" title="4"></a>
<a class="sourceLine" id="cb14-5" title="5"><span class="ot">def ::</span> <span class="dt">Parser</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb14-6" title="6">def <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb14-7" title="7">  reserved <span class="st">&quot;def&quot;</span></a>
<a class="sourceLine" id="cb14-8" title="8">  name <span class="ot">&lt;-</span> identifier</a>
<a class="sourceLine" id="cb14-9" title="9">  args <span class="ot">&lt;-</span> parens (commaSep identifier)</a>
<a class="sourceLine" id="cb14-10" title="10">  body <span class="ot">&lt;-</span> absBlockExpr</a>
<a class="sourceLine" id="cb14-11" title="11">  <span class="fu">return</span> (<span class="dt">Func</span> name args body)</a></code></pre></div>
<p>We define a function absBlockExpr. You’ll notice we also used a <code>localIndentation</code>. The reason for that is <code>absolutionIndentation</code> normally defaults to the first token of the parent. In our case, this is <code>def</code> and we want instead for it to choose <code>add</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb15-1" title="1">parse test3</a>
<a class="sourceLine" id="cb15-2" title="2"><span class="co">-- Left &quot;[input]&quot; (line 3, column 3):</span></a>
<a class="sourceLine" id="cb15-3" title="3"><span class="co">-- expecting identifier</span></a>
<a class="sourceLine" id="cb15-4" title="4"><span class="co">--</span></a>
<a class="sourceLine" id="cb15-5" title="5"><span class="co">-- Invalid indentation.</span></a>
<a class="sourceLine" id="cb15-6" title="6"><span class="co">--   Found a token at indentation 2.</span></a>
<a class="sourceLine" id="cb15-7" title="7"><span class="co">--   Expecting a token at an indentation greater than or equal to 5.</span></a>
<a class="sourceLine" id="cb15-8" title="8"><span class="co">--   IndentStream { indentationState =</span></a>
<a class="sourceLine" id="cb15-9" title="9"><span class="co">--                   IndentationState { minIndentation = 5</span></a>
<a class="sourceLine" id="cb15-10" title="10"><span class="co">--                                    , maxIndentation = 5</span></a>
<a class="sourceLine" id="cb15-11" title="11"><span class="co">--                                    , absMode = False</span></a>
<a class="sourceLine" id="cb15-12" title="12"><span class="co">--                                    , tokenRel = Ge}</span></a>
<a class="sourceLine" id="cb15-13" title="13"><span class="co">--                , tokenStream = &quot;&quot;}</span></a></code></pre></div>
<p>Now it works as expected</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb16-1" title="1">test4 <span class="ot">=</span> <span class="fu">unlines</span></a>
<a class="sourceLine" id="cb16-2" title="2">  [ <span class="st">&quot;def foo(x, y):&quot;</span></a>
<a class="sourceLine" id="cb16-3" title="3">  , <span class="st">&quot;    add x&quot;</span></a>
<a class="sourceLine" id="cb16-4" title="4">  , <span class="st">&quot;     y&quot;</span></a>
<a class="sourceLine" id="cb16-5" title="5">  ]</a>
<a class="sourceLine" id="cb16-6" title="6"></a>
<a class="sourceLine" id="cb16-7" title="7">parse test4</a>
<a class="sourceLine" id="cb16-8" title="8"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a>
<a class="sourceLine" id="cb16-9" title="9">parse test1</a>
<a class="sourceLine" id="cb16-10" title="10"><span class="co">-- Right [Func &quot;foo&quot; [&quot;x&quot;,&quot;y&quot;] (Add (Var &quot;x&quot;) (Var &quot;y&quot;))]</span></a></code></pre></div>
<p>This library has other bits to it, but this should give enough to figure out, how to add indentation sensitivity to your language.</p>
<p>Special thanks to <a href="http://www.lambdageek.org/aleksey/">Aleksey Kliger</a> for helping me understand this library.</p>

</div>

<div id="disqus_thread"></div>
<script>
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://convexoptimized.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Tue, 12 Jan 2016 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2016-01-12-indentation-sensitive-parsing/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>
<item>
    <title>Building a probabilistic programming interpreter</title>
    <link>http://zinkov.com/posts/2015-08-25-building-a-probabilisitic-interpreter/index.html</link>
    <description><![CDATA[<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Building a probabilistic programming interpreter</title>
        <style>

	  html body {
            font-family: 'Montserrat', sans-serif;
            background-color: white;
	  }

	  :root {
            --accent: #002147;
            --border-width: 5px ;
	  }
	  
        </style>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,600">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

<link rel="feed" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="../../rss.xml" />

<link rel="stylesheet" href="../../css/minimal.css">
<link rel="stylesheet" type="text/css" href="../../css/code2.css" />



<script type="text/javascript">
function toggle(id) {
    el = document.getElementById(id);
    if (el.style.display == 'none') {
	el.style.display = 'block';
    } else {
	el.style.display = 'none';
    }
}
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
          extensions: ["AMSmath.js"],
          Macros: {
              Expect: '{\\mathbb E}',
              real: '{\\mathbb R}',
              v: ['{\\mathbf{#1}}',1],
          }
      },
      tex2jax: {
	  inlineMath: [	 ["\\(","\\)"], ],
	  displayMath: [  ["\\[","\\]"] ]
      },
      displayAlign: 'center', // Change this to 'center' to center equations.
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 4}}
      }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body>

  <nav class="navbar navbar-default" role="navigation">
    <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../">Convex Optimized</a>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#bs-navbar">
	<span class="sr-only">Toggle navigation</span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
	<span class="icon-bar"></span>
      </button>
    </div>
    <div id="bs-navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
	<li><a href="../../archive/">Archive</a></li>
	<li><a href="../../pubs/">Publications</a></li>
	<li><a href="../../about/">About</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
    <li class="navbar-icon">
      <a href="mailto:com.zinkov@rob"><i class="fa fa-envelope-o"></i></a>
    </li>
	<li class="navbar-icon">
      <a href="https://github.com/zaxtax/" rel="me"><i class="fa fa-github"></i></a>
    </li>
	<li class="navbar-icon"><a href="https://twitter.com/zaxtax/" rel="me">
	<i class="fa fa-twitter"></i></a>
    </li>
      </ul>
    </div>
    </div>
  </nav>

  <main>
  <div class="item">
  <div class="item">
<h2>Building a probabilistic programming interpreter</h2>
<h5><strong>Rob Zinkov</strong></h5>
<h5>2015-08-25</h5>
</div>

<div class="text-left">

<p>Very often interpreters for probabilisitic programming languages (PPLs) can seem a little mysterious. In actuality, if you know how to write an interpreter for a simple language it isn’t that much more work.</p>
<p>Using Haskell as the host language I’ll show how to write a simple PPL which uses importance sampling as the underlying inference method. There is nothing special about using from haskell other than pattern-matching so this example should be pretty easy to port to other languages.</p>
<p>To start let’s import some things and set up some basic types</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">import</span> <span class="dt">Data.List</span> <span class="kw">hiding</span> (empty, insert, map)</a>
<a class="sourceLine" id="cb1-2" title="2"><span class="kw">import</span> <span class="dt">Control.Monad</span></a>
<a class="sourceLine" id="cb1-3" title="3"></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="kw">import</span> <span class="dt">Data.HashMap.Strict</span> <span class="kw">hiding</span> (map)</a>
<a class="sourceLine" id="cb1-5" title="5"><span class="kw">import</span> <span class="dt">System.Random.MWC</span> <span class="kw">as</span> <span class="dt">MWC</span></a>
<a class="sourceLine" id="cb1-6" title="6"><span class="kw">import</span> <span class="dt">System.Random.MWC.Distributions</span> <span class="kw">as</span> <span class="dt">MD</span></a>
<a class="sourceLine" id="cb1-7" title="7"></a>
<a class="sourceLine" id="cb1-8" title="8"><span class="kw">type</span> <span class="dt">Name</span> <span class="ot">=</span> <span class="dt">String</span></a>
<a class="sourceLine" id="cb1-9" title="9"><span class="kw">type</span> <span class="dt">Env</span>  <span class="ot">=</span> <span class="dt">HashMap</span> <span class="dt">String</span> <span class="dt">Val</span></a></code></pre></div>
<p>Our language will have as values functions, doubles, bools and pairs of those.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" title="1"><span class="kw">data</span> <span class="dt">Val</span> <span class="ot">=</span></a>
<a class="sourceLine" id="cb2-2" title="2">    <span class="dt">D</span> <span class="dt">Double</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb2-3" title="3">    <span class="dt">B</span> <span class="dt">Bool</span>   <span class="op">|</span></a>
<a class="sourceLine" id="cb2-4" title="4">    <span class="dt">F</span> (<span class="dt">Val</span> <span class="ot">-&gt;</span> <span class="dt">Val</span>) <span class="op">|</span></a>
<a class="sourceLine" id="cb2-5" title="5">    <span class="dt">P</span> <span class="dt">Val</span> <span class="dt">Val</span></a>
<a class="sourceLine" id="cb2-6" title="6"></a>
<a class="sourceLine" id="cb2-7" title="7"><span class="kw">instance</span> <span class="dt">Eq</span> <span class="dt">Val</span> <span class="kw">where</span></a>
<a class="sourceLine" id="cb2-8" title="8">  <span class="dt">D</span> x <span class="op">==</span> <span class="dt">D</span> y         <span class="ot">=</span> x <span class="op">==</span> y</a>
<a class="sourceLine" id="cb2-9" title="9">  <span class="dt">B</span> x <span class="op">==</span> <span class="dt">B</span> y         <span class="ot">=</span> x <span class="op">==</span> y</a>
<a class="sourceLine" id="cb2-10" title="10">  <span class="dt">P</span> x1 x2 <span class="op">==</span> <span class="dt">P</span> y1 y2 <span class="ot">=</span> x1 <span class="op">==</span> y1 <span class="op">&amp;&amp;</span> x2 <span class="op">==</span> y2</a>
<a class="sourceLine" id="cb2-11" title="11">  _ <span class="op">==</span> _             <span class="ot">=</span> <span class="dt">False</span></a>
<a class="sourceLine" id="cb2-12" title="12"></a>
<a class="sourceLine" id="cb2-13" title="13"><span class="kw">instance</span> <span class="dt">Ord</span> <span class="dt">Val</span> <span class="kw">where</span></a>
<a class="sourceLine" id="cb2-14" title="14">  <span class="dt">D</span> x <span class="op">&lt;=</span> <span class="dt">D</span> y         <span class="ot">=</span> x <span class="op">&lt;=</span> y</a>
<a class="sourceLine" id="cb2-15" title="15">  <span class="dt">B</span> x <span class="op">&lt;=</span> <span class="dt">B</span> y         <span class="ot">=</span> x <span class="op">&lt;=</span> y</a>
<a class="sourceLine" id="cb2-16" title="16">  <span class="dt">P</span> x1 x2 <span class="op">&lt;=</span> <span class="dt">P</span> y1 y2 <span class="ot">=</span> x1 <span class="op">&lt;=</span> y1 <span class="op">&amp;&amp;</span> x2 <span class="op">&lt;=</span> y2</a>
<a class="sourceLine" id="cb2-17" title="17">  _ <span class="op">&lt;=</span> _             <span class="ot">=</span> <span class="fu">error</span> <span class="st">&quot;Comparing functions is undefined&quot;</span></a></code></pre></div>
<p>This language will have expressions for these values, conditionals and arithmetic.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">data</span> <span class="dt">Expr</span> <span class="ot">=</span></a>
<a class="sourceLine" id="cb3-2" title="2">     <span class="dt">Lit</span> <span class="dt">Double</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-3" title="3">     <span class="dt">Var</span> <span class="dt">Name</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-4" title="4">     <span class="dt">Pair</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-5" title="5">     <span class="dt">Fst</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-6" title="6">     <span class="dt">Snd</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-7" title="7">     <span class="dt">If</span>  <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-8" title="8"></a>
<a class="sourceLine" id="cb3-9" title="9">     <span class="dt">Eql</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-10" title="10">     <span class="dt">Les</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-11" title="11">     <span class="dt">Gre</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-12" title="12">     <span class="dt">And</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-13" title="13"></a>
<a class="sourceLine" id="cb3-14" title="14">     <span class="dt">Lam</span> <span class="dt">Name</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-15" title="15">     <span class="dt">App</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-16" title="16"></a>
<a class="sourceLine" id="cb3-17" title="17">     <span class="dt">Add</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-18" title="18">     <span class="dt">Sub</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-19" title="19">     <span class="dt">Mul</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb3-20" title="20">     <span class="dt">Div</span> <span class="dt">Expr</span> <span class="dt">Expr</span></a>
<a class="sourceLine" id="cb3-21" title="21"> <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)</a></code></pre></div>
<p>We can evalute expressions in this language without doing anything special.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" title="1"><span class="ot">evalT ::</span> <span class="dt">Expr</span> <span class="ot">-&gt;</span> <span class="dt">Env</span> <span class="ot">-&gt;</span> <span class="dt">Val</span></a>
<a class="sourceLine" id="cb4-2" title="2">evalT (<span class="dt">Lit</span> a) _            <span class="ot">=</span> <span class="dt">D</span> a</a>
<a class="sourceLine" id="cb4-3" title="3">evalT (<span class="dt">Var</span> x)      env     <span class="ot">=</span> env <span class="op">!</span> x</a>
<a class="sourceLine" id="cb4-4" title="4">evalT (<span class="dt">Lam</span> x body) env     <span class="ot">=</span> <span class="dt">F</span> (\ x' <span class="ot">-&gt;</span> evalT body (insert x x' env))</a>
<a class="sourceLine" id="cb4-5" title="5">evalT (<span class="dt">App</span> f x)    env     <span class="ot">=</span> app (evalT f env) (evalT x env)</a>
<a class="sourceLine" id="cb4-6" title="6">           </a>
<a class="sourceLine" id="cb4-7" title="7">evalT (<span class="dt">Eql</span> x y)    env     <span class="ot">=</span> <span class="dt">B</span> <span class="op">$</span> (evalT x env) <span class="op">==</span> (evalT y env)</a>
<a class="sourceLine" id="cb4-8" title="8">evalT (<span class="dt">Les</span> x y)    env     <span class="ot">=</span> <span class="dt">B</span> <span class="op">$</span> (evalT x env) <span class="op">&lt;=</span> (evalT y env)</a>
<a class="sourceLine" id="cb4-9" title="9">evalT (<span class="dt">Gre</span> x y)    env     <span class="ot">=</span> <span class="dt">B</span> <span class="op">$</span> (evalT x env) <span class="op">&gt;=</span> (evalT y env)</a>
<a class="sourceLine" id="cb4-10" title="10">evalT (<span class="dt">And</span> x y)    env     <span class="ot">=</span> liftB (<span class="op">&amp;&amp;</span>) (evalT x env) (evalT y env)</a>
<a class="sourceLine" id="cb4-11" title="11">                </a>
<a class="sourceLine" id="cb4-12" title="12">evalT (<span class="dt">Add</span> x y)    env     <span class="ot">=</span> liftOp (<span class="op">+</span>) (evalT x env) (evalT y env)</a>
<a class="sourceLine" id="cb4-13" title="13">evalT (<span class="dt">Sub</span> x y)    env     <span class="ot">=</span> liftOp (<span class="op">-</span>) (evalT x env) (evalT y env)</a>
<a class="sourceLine" id="cb4-14" title="14">evalT (<span class="dt">Mul</span> x y)    env     <span class="ot">=</span> liftOp (<span class="op">*</span>) (evalT x env) (evalT y env)</a>
<a class="sourceLine" id="cb4-15" title="15">evalT (<span class="dt">Div</span> x y)    env     <span class="ot">=</span> liftOp (<span class="op">/</span>) (evalT x env) (evalT y env)</a>
<a class="sourceLine" id="cb4-16" title="16">                           </a>
<a class="sourceLine" id="cb4-17" title="17">evalT (<span class="dt">Pair</span> x y)   env     <span class="ot">=</span> <span class="dt">P</span> (evalT x env) (evalT y env)</a>
<a class="sourceLine" id="cb4-18" title="18">evalT (<span class="dt">Fst</span> x)      env     <span class="ot">=</span> fst_ <span class="op">$</span> evalT x env</a>
<a class="sourceLine" id="cb4-19" title="19"> <span class="kw">where</span> fst_ (<span class="dt">P</span> a b)        <span class="ot">=</span> a</a>
<a class="sourceLine" id="cb4-20" title="20">evalT (<span class="dt">Snd</span> x)      env     <span class="ot">=</span> snd_ <span class="op">$</span> evalT x env</a>
<a class="sourceLine" id="cb4-21" title="21"> <span class="kw">where</span> snd_ (<span class="dt">P</span> a b)        <span class="ot">=</span> b</a>
<a class="sourceLine" id="cb4-22" title="22">evalT (<span class="dt">If</span> b t f)   env     <span class="ot">=</span> if_ (evalT b env) (evalT t env) (evalT f env)</a>
<a class="sourceLine" id="cb4-23" title="23"> <span class="kw">where</span> if_ (<span class="dt">B</span> <span class="dt">True</span>)  t' f' <span class="ot">=</span> t'</a>
<a class="sourceLine" id="cb4-24" title="24">       if_ (<span class="dt">B</span> <span class="dt">False</span>) t' f' <span class="ot">=</span> f'</a>
<a class="sourceLine" id="cb4-25" title="25"></a>
<a class="sourceLine" id="cb4-26" title="26"><span class="ot">app ::</span> <span class="dt">Val</span> <span class="ot">-&gt;</span> <span class="dt">Val</span> <span class="ot">-&gt;</span> <span class="dt">Val</span></a>
<a class="sourceLine" id="cb4-27" title="27">app (<span class="dt">F</span> f') x'   <span class="ot">=</span> f' x'</a>
<a class="sourceLine" id="cb4-28" title="28"></a>
<a class="sourceLine" id="cb4-29" title="29"><span class="ot">liftOp ::</span> (<span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Double</span>) <span class="ot">-&gt;</span></a>
<a class="sourceLine" id="cb4-30" title="30">          <span class="dt">Val</span>     <span class="ot">-&gt;</span> <span class="dt">Val</span>    <span class="ot">-&gt;</span> <span class="dt">Val</span></a>
<a class="sourceLine" id="cb4-31" title="31">liftOp op (<span class="dt">D</span> e1) (<span class="dt">D</span> e2) <span class="ot">=</span> <span class="dt">D</span> (op e1 e2)</a>
<a class="sourceLine" id="cb4-32" title="32"></a>
<a class="sourceLine" id="cb4-33" title="33"><span class="ot">liftB  ::</span> (<span class="dt">Bool</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span>) <span class="ot">-&gt;</span></a>
<a class="sourceLine" id="cb4-34" title="34">          <span class="dt">Val</span>     <span class="ot">-&gt;</span> <span class="dt">Val</span>    <span class="ot">-&gt;</span> <span class="dt">Val</span></a>
<a class="sourceLine" id="cb4-35" title="35">liftB  op (<span class="dt">B</span> e1) (<span class="dt">B</span> e2) <span class="ot">=</span> <span class="dt">B</span> (op e1 e2)</a></code></pre></div>
<p>Of course this isn’t a probabilisitic programming language. So now we extend our language to include measures.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" title="1"><span class="kw">data</span> <span class="dt">Meas</span> <span class="ot">=</span></a>
<a class="sourceLine" id="cb5-2" title="2">     <span class="dt">Uniform</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb5-3" title="3">     <span class="dt">Weight</span>  <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb5-4" title="4">     <span class="dt">Bind</span> <span class="dt">Name</span> <span class="dt">Meas</span> <span class="dt">Meas</span></a>
<a class="sourceLine" id="cb5-5" title="5"> <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)</a></code></pre></div>
<p>Let’s take a moment to explain what makes something a measure. Measures can considered un-normalized probability distributions. If you take the sum of the probability of each disjoint outcome from a un-normalized probability distribution, the answer may not be 1.</p>
<p>This is relevant as we will be representing measures as a list of weighted draws from the underlying distribution. Those draws will need to be normalized to be understood as a probability distribution.</p>
<p>We can construct measures in one of three ways. We may simply have the continuous uniform distribution whose bounds are defined as expressions. We may have a weighted distribution which only returns the value of its second argument, with probability of the first argument. This is only a probability distribution when the first argument evaluates to one. We’ll call this case <code>dirac</code></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" title="1"><span class="ot">dirac ::</span> <span class="dt">Expr</span> <span class="ot">-&gt;</span> <span class="dt">Meas</span></a>
<a class="sourceLine" id="cb6-2" title="2">dirac x <span class="ot">=</span> <span class="dt">Weight</span> (<span class="dt">Lit</span> <span class="fl">1.0</span>) x</a></code></pre></div>
<p>The final form is what let’s us build measure expressions. What <code>Bind</code> does is take a measure as input, and a function from draws in that measure to another measure.</p>
<p>Because I don’t feel like defining measurable functions in their own form, <code>Bind</code> also takes a name to set what variable will hold values forthe draws, so the last argument to bind may just use that variable when it wants to refer to those draws. As an example if I wish to take a draw from a uniform distribution and then square that value.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" title="1">prog1 <span class="ot">=</span> <span class="dt">Bind</span> <span class="st">&quot;x&quot;</span> (<span class="dt">Uniform</span> (<span class="dt">Lit</span> <span class="dv">1</span>) (<span class="dt">Lit</span> <span class="dv">5</span>))   <span class="co">-- x &lt;~ uniform(1, 5)</span></a>
<a class="sourceLine" id="cb7-2" title="2">        (dirac (<span class="dt">Add</span> (<span class="dt">Var</span> <span class="st">&quot;x&quot;</span>) (<span class="dt">Var</span> <span class="st">&quot;x&quot;</span>)))   <span class="co">-- return (x + x)</span></a></code></pre></div>
<p>Measures are evaluated by producing a weighted sample from the measure space they represent. This is also called importance sampling.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb8-1" title="1"><span class="ot">evalM ::</span> <span class="dt">Meas</span> <span class="ot">-&gt;</span> <span class="dt">Env</span> <span class="ot">-&gt;</span> <span class="dt">MWC.GenIO</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Val</span>, <span class="dt">Double</span>)</a>
<a class="sourceLine" id="cb8-2" title="2">evalM (<span class="dt">Uniform</span> lo hi) env g <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb8-3" title="3">                              <span class="kw">let</span> <span class="dt">D</span> lo' <span class="ot">=</span> evalT lo env</a>
<a class="sourceLine" id="cb8-4" title="4">                              <span class="kw">let</span> <span class="dt">D</span> hi' <span class="ot">=</span> evalT hi env</a>
<a class="sourceLine" id="cb8-5" title="5">                              x <span class="ot">&lt;-</span> MWC.uniformR (lo', hi') g</a>
<a class="sourceLine" id="cb8-6" title="6">                              <span class="fu">return</span> (<span class="dt">D</span> x, <span class="fl">1.0</span>)</a>
<a class="sourceLine" id="cb8-7" title="7">evalM (<span class="dt">Weight</span> i x)    env g <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb8-8" title="8">                              <span class="kw">let</span> <span class="dt">D</span> i' <span class="ot">=</span> evalT i env</a>
<a class="sourceLine" id="cb8-9" title="9">                              <span class="fu">return</span> (evalT x env, i')</a>
<a class="sourceLine" id="cb8-10" title="10">evalM (<span class="dt">Bind</span> x m f)    env g <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb8-11" title="11">                              (x', w)  <span class="ot">&lt;-</span> evalM m env g</a>
<a class="sourceLine" id="cb8-12" title="12">                              <span class="kw">let</span> env' <span class="ot">=</span> insert x x' env</a>
<a class="sourceLine" id="cb8-13" title="13">                              (f', w1) <span class="ot">&lt;-</span> evalM f env' g</a>
<a class="sourceLine" id="cb8-14" title="14">                              <span class="fu">return</span> (f', w<span class="op">*</span>w1)</a></code></pre></div>
<p>We may run these programs as follows</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb9-1" title="1"><span class="ot">test1 ::</span> <span class="dt">IO</span> ()</a>
<a class="sourceLine" id="cb9-2" title="2">test1 <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb9-3" title="3">   g <span class="ot">&lt;-</span> MWC.create</a>
<a class="sourceLine" id="cb9-4" title="4">   draw <span class="ot">&lt;-</span> evalM prog1 empty g</a>
<a class="sourceLine" id="cb9-5" title="5">   <span class="fu">print</span> draw</a>
<a class="sourceLine" id="cb9-6" title="6"></a>
<a class="sourceLine" id="cb9-7" title="7">(<span class="fl">7.926912543562406</span>,<span class="fl">1.0</span>)</a></code></pre></div>
<p>Evaluating this program repeatedly will allow you to produce as many draws from this measure as you need. This is great in that we can represent any unconditioned probability distribution. But how do we represent conditional distributions?</p>
<p>For that we will introduce another datatype</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb10-1" title="1"><span class="kw">data</span> <span class="dt">Cond</span> <span class="ot">=</span></a>
<a class="sourceLine" id="cb10-2" title="2">    <span class="dt">UCond</span> <span class="dt">Meas</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb10-3" title="3">    <span class="dt">UniformC</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb10-4" title="4">    <span class="dt">WeightC</span>  <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb10-5" title="5">    <span class="dt">BindC</span> <span class="dt">Name</span> <span class="dt">Cond</span> <span class="dt">Cond</span></a></code></pre></div>
<p>This is just an extension of <code>Meas</code> expect now we may say, a measure is either unconditioned, or if its conditioned for each case we may specify additionally which value its conditioned on. To draw from a conditioned measure, we convert it into an unconditional measure.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb11-1" title="1"><span class="ot">evalC ::</span> <span class="dt">Cond</span> <span class="ot">-&gt;</span> <span class="dt">Meas</span></a>
<a class="sourceLine" id="cb11-2" title="2">evalC (<span class="dt">UCond</span>    m      ) <span class="ot">=</span> m</a>
<a class="sourceLine" id="cb11-3" title="3">evalC (<span class="dt">UniformC</span> lo hi x) <span class="ot">=</span> <span class="dt">Weight</span> (<span class="dt">If</span> (<span class="dt">And</span> (<span class="dt">Gre</span> x lo)</a>
<a class="sourceLine" id="cb11-4" title="4">                                                 (<span class="dt">Les</span> x hi))</a>
<a class="sourceLine" id="cb11-5" title="5">                                         (<span class="dt">Div</span> x (<span class="dt">Sub</span> hi lo))</a>
<a class="sourceLine" id="cb11-6" title="6">                                         (<span class="dt">Lit</span> <span class="dv">0</span>)) x</a>
<a class="sourceLine" id="cb11-7" title="7">evalC (<span class="dt">WeightC</span>  i x   y) <span class="ot">=</span> <span class="dt">Weight</span> (<span class="dt">If</span> (<span class="dt">Eql</span> x y)</a>
<a class="sourceLine" id="cb11-8" title="8">                                         i</a>
<a class="sourceLine" id="cb11-9" title="9">                                         (<span class="dt">Lit</span> <span class="dv">0</span>)) y</a>
<a class="sourceLine" id="cb11-10" title="10">evalC (<span class="dt">BindC</span>    x m f)   <span class="ot">=</span> <span class="dt">Bind</span> x (evalC m) (evalC f)</a></code></pre></div>
<p>What <code>evalC</code> does is determine what weight to assign to a measure given we know it will produce a particular value. This weight is the probability of getting this value from the measure.</p>
<p>And that’s all you need to express probabilisitic programs. Take the following example. Suppose we have two random variables <code>x</code> and <code>y</code> where the value of <code>y</code> depends on <code>x</code></p>
<pre><code>x &lt;~ uniform(1, 5)
y &lt;~ uniform(x, 7)</code></pre>
<p>What’s the conditional distribution on <code>x</code> given <code>y</code> is <code>3</code>?</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb13-1" title="1">prog2 <span class="ot">=</span> <span class="dt">BindC</span> <span class="st">&quot;x&quot;</span> (<span class="dt">UCond</span> (<span class="dt">Uniform</span> (<span class="dt">Lit</span> <span class="dv">1</span>) (<span class="dt">Lit</span> <span class="dv">5</span>)))      <span class="co">-- x &lt;~ uniform(1, 5)</span></a>
<a class="sourceLine" id="cb13-2" title="2">         (<span class="dt">BindC</span> <span class="st">&quot;_&quot;</span> (<span class="dt">UniformC</span> (<span class="dt">Var</span> <span class="st">&quot;x&quot;</span>) (<span class="dt">Lit</span> <span class="dv">7</span>) (<span class="dt">Lit</span> <span class="dv">3</span>)) <span class="co">-- y &lt;~ uniform(x, 7)</span></a>
<a class="sourceLine" id="cb13-3" title="3">                                                         <span class="co">-- observe y 3</span></a>
<a class="sourceLine" id="cb13-4" title="4">          (<span class="dt">UCond</span> (dirac (<span class="dt">Var</span> <span class="st">&quot;x&quot;</span>))))                     <span class="co">-- return x</span></a>
<a class="sourceLine" id="cb13-5" title="5"></a>
<a class="sourceLine" id="cb13-6" title="6"><span class="ot">test2 ::</span> <span class="dt">IO</span> ()</a>
<a class="sourceLine" id="cb13-7" title="7">test2 <span class="ot">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb13-8" title="8">   g <span class="ot">&lt;-</span> MWC.create</a>
<a class="sourceLine" id="cb13-9" title="9">   samples <span class="ot">&lt;-</span> replicateM <span class="dv">10</span> (evalM (evalC prog2) empty g)</a>
<a class="sourceLine" id="cb13-10" title="10">   <span class="fu">print</span> samples</a>
<a class="sourceLine" id="cb13-11" title="11"></a>
<a class="sourceLine" id="cb13-12" title="12">[(<span class="fl">1.099241451531848</span>, <span class="fl">0.5084092113511076</span>),</a>
<a class="sourceLine" id="cb13-13" title="13"> (<span class="fl">3.963456271781203</span>, <span class="fl">0.0</span>),</a>
<a class="sourceLine" id="cb13-14" title="14"> (<span class="fl">1.637454187135532</span>, <span class="fl">0.5594357800735532</span>),</a>
<a class="sourceLine" id="cb13-15" title="15"> (<span class="fl">3.781075065891581</span>, <span class="fl">0.0</span>),</a>
<a class="sourceLine" id="cb13-16" title="16"> (<span class="fl">1.908186342514358</span>, <span class="fl">0.5891810269980327</span>),</a>
<a class="sourceLine" id="cb13-17" title="17"> (<span class="fl">2.799366130116895</span>, <span class="fl">0.714177929552209</span>),</a>
<a class="sourceLine" id="cb13-18" title="18"> (<span class="fl">3.091757816253942</span>, <span class="fl">0.0</span>),</a>
<a class="sourceLine" id="cb13-19" title="19"> (<span class="fl">1.486166046469419</span>, <span class="fl">0.5440860253107659</span>),</a>
<a class="sourceLine" id="cb13-20" title="20"> (<span class="fl">3.106369061983323</span>, <span class="fl">0.0</span>),</a>
<a class="sourceLine" id="cb13-21" title="21"> (<span class="fl">1.225163855492708</span>, <span class="fl">0.5194952592470413</span>)]</a></code></pre></div>
<p>As you can see, anything above <code>3</code> for <code>x</code> has a weight of <code>0</code> because it would be impossible for to observe <code>y</code> with <code>3</code>.</p>
<h2 id="further-reading">Further reading</h2>
<p>This implementation for small problems is actually fairly capable. It can be extended to support more probability distributions in a straightforward way.</p>
<p>If you are interested in more advanced interpreters I suggest reading the following.</p>
<ul>
<li><a href="http://www.dippl.org">The Design and Implementation of Probabilistic Programming Languages</a></li>
<li><a href="http://www.mit.edu/~ast/papers/lightweight-mcmc-aistats2011.pdf">Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation</a></li>
<li><a href="http://arxiv.org/abs/1507.00996">A New Approach to Probabilistic Programming Inference</a></li>
<li><a href="http://arxiv.org/abs/1403.0504">A Compilation Target for Probabilistic Programming Languages</a></li>
</ul>

</div>

<div id="disqus_thread"></div>
<script>
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');

        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://convexoptimized.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>

  </div>
  </main>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap.native/2.0.15/bootstrap-native.min.js"></script>
  </body>
</html>
]]></description>
    <pubDate>Tue, 25 Aug 2015 00:00:00 UT</pubDate>
    <guid>http://zinkov.com/posts/2015-08-25-building-a-probabilisitic-interpreter/index.html</guid>
    <dc:creator>Rob Zinkov</dc:creator>
</item>

    </channel>
</rss>
